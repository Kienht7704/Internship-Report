[{"uri":"https://Kienht7704.github.io/Internship-Report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://Kienht7704.github.io/Internship-Report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://Kienht7704.github.io/Internship-Report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://Kienht7704.github.io/Internship-Report/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://Kienht7704.github.io/Internship-Report/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://Kienht7704.github.io/Internship-Report/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://Kienht7704.github.io/Internship-Report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Huynh Trung Kien\nPhone Number: 0964325416\nEmail: kienhtse184454@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to \u0026hellip;\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://Kienht7704.github.io/Internship-Report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Workshop Overview In this workshop you will build a complete personal portfolio website using a serverless architecture on AWS ‚Äî no server management or scaling worries required.\nWorkshop Architecture The workshop leverages two main AWS services:\nAmazon S3 (Simple Storage Service) ‚Äî serves as the static web server for files such as HTML, CSS and images. S3 Static Website Hosting allows low-cost hosting and virtually unlimited storage.\nAmazon CloudFront ‚Äî a Content Delivery Network (CDN) that accelerates your site by delivering content from the edge location nearest to the user, instead of fetching it from a centralized origin. CloudFront also provides free HTTPS via AWS Certificate Manager.\nBenefits of this Architecture Performance: Content is served from the edge location closest to the user, reducing latency and improving page load times.\nCost: With the AWS Free Tier you get 12 months that include 5 GB of S3 storage, 1 TB of CloudFront transfer, and 10 million requests. After the Free Tier, hosting costs are typically very low (roughly ~$0.60/month for light traffic).\nOperations: No server management required ‚Äî no patching or manual scaling. AWS handles availability and scaling for you.\nSecurity: HTTPS is available by default, S3 bucket policies control access, and CloudFront includes AWS Shield Standard for basic DDoS protection.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/5-workshop/5.2-workshop-module1/","title":"Module 1","tags":[],"description":"","content":"Guide: Create Website Files for the Portfolio Objective In this module you will create two basic HTML files for your portfolio website:\nindex.html - the home page showing personal information error.html - the 404 error page when users access a non-existing path Step 1: Create a project folder Open File Explorer (Windows) or Finder (Mac). Create a new folder named my-portfolio. Open that folder with your text editor (VS Code, Notepad++, Sublime Text, etc.). Step 2: Create the index.html file 2.1. Create a new file Inside the my-portfolio folder, create a new file named index.html. 2.2. Copy the code into the file Open index.html and paste the following code:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Portfolio - Your Name\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; * { margin: 0; padding: 0; box-sizing: border-box; } body { font-family: \u0026#39;Segoe UI\u0026#39;, Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #333; } .container { max-width: 1200px; margin: 0 auto; padding: 0 20px; } header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 100px 0; text-align: center; } header h1 { font-size: 3em; margin-bottom: 10px; } header p { font-size: 1.2em; opacity: 0.9; } .section { padding: 60px 0; } .section:nth-child(even) { background: #f8f9fa; } h2 { color: #667eea; margin-bottom: 30px; font-size: 2em; } .skills { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin-top: 30px; } .skill-card { background: white; padding: 30px; border-radius: 10px; box-shadow: 0 5px 15px rgba(0,0,0,0.1); transition: transform 0.3s; } .skill-card:hover { transform: translateY(-5px); } .skill-card h3 { color: #667eea; margin-bottom: 10px; } .projects { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 30px; margin-top: 30px; } .project-card { background: white; border-radius: 10px; overflow: hidden; box-shadow: 0 5px 15px rgba(0,0,0,0.1); } .project-image { height: 200px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); display: flex; align-items: center; justify-content: center; color: white; font-size: 3em; } .project-content { padding: 20px; } .project-content h3 { color: #667eea; margin-bottom: 10px; } footer { background: #333; color: white; text-align: center; padding: 30px 0; } .contact-btn { display: inline-block; background: white; color: #667eea; padding: 12px 30px; border-radius: 25px; text-decoration: none; margin-top: 20px; font-weight: bold; transition: transform 0.3s; } .contact-btn:hover { transform: scale(1.05); } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;header\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Hello, I\u0026#39;m [Your Name]\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;AWS Solutions Architect | Cloud Engineer\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;section class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;About Me\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;I am a cloud engineer passionate about AWS, specializing in designing and deploying scalable cloud solutions. With experience working with EC2, S3, Lambda, and other AWS services, I am always looking to learn and apply new technologies.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;section class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Skills\u0026lt;/h2\u0026gt; \u0026lt;div class=\u0026#34;skills\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;skill-card\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;AWS Services\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;EC2, S3, Lambda, RDS, VPC, CloudFront, Route 53, IAM\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;skill-card\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Infrastructure as Code\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Terraform, CloudFormation, AWS CDK\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;skill-card\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Containers \u0026amp; Orchestration\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Docker, Kubernetes, ECS, EKS\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;skill-card\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Programming\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Python, JavaScript, Bash, SQL\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;section class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Featured Projects\u0026lt;/h2\u0026gt; \u0026lt;div class=\u0026#34;projects\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;project-card\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;project-image\u0026#34;\u0026gt;üåê\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;project-content\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Website Hosting on S3\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Deploy a static website with S3, CloudFront, and a custom domain. Low cost (\u0026amp;lt; $1/month) and fast load times (\u0026amp;lt; 1s).\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;project-card\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;project-image\u0026#34;\u0026gt;‚ö°\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;project-content\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Serverless API\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Build a RESTful API using Lambda, API Gateway, and DynamoDB. Auto-scaling with no server management.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;project-card\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;project-image\u0026#34;\u0026gt;üîê\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;project-content\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Security Automation\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Automate security audits using Lambda, CloudWatch, and SNS. Early detection and alerts for security issues.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;footer\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Contact\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Email: your.email@example.com\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;GitHub: github.com/yourusername\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;LinkedIn: linkedin.com/in/yourusername\u0026lt;/p\u0026gt; \u0026lt;a href=\u0026#34;mailto:your.email@example.com\u0026#34; class=\u0026#34;contact-btn\u0026#34;\u0026gt;Send Email\u0026lt;/a\u0026gt; \u0026lt;p style=\u0026#34;margin-top: 30px; opacity: 0.7;\u0026#34;\u0026gt;¬© 2024 - Hosted on AWS S3 + CloudFront\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 2.3. Customize personal information Replace the following with your own details:\nLine with [Your Name] ‚Üí replace with your name Job title line About section paragraph Contact info (email, GitHub, LinkedIn) 2.4. Save the file Use Ctrl + S (Windows) or Cmd + S (Mac) to save.\nStep 3: Create the error.html file 3.1. Create a new file In the same my-portfolio folder, create a file named error.html. 3.2. Copy the code into the file \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;404 - Page Not Found\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { font-family: Arial, sans-serif; display: flex; justify-content: center; align-items: center; height: 100vh; margin: 0; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; text-align: center; } h1 { font-size: 5em; margin: 0; } p { font-size: 1.5em; } a { color: white; text-decoration: underline; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h1\u0026gt;404\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Oops! The page you\u0026#39;re looking for cannot be found.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;/\u0026#34;\u0026gt;Return to the home page\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 3.3. Save the file Use Ctrl + S (Windows) or Cmd + S (Mac).\nStep 4: Quick local check Open File Explorer/Finder and locate index.html and error.html inside my-portfolio. Double-click a file to open it in your default browser and verify layout and content. Step 5: Final folder structure my-portfolio/ ‚îú‚îÄ‚îÄ index.html ‚îî‚îÄ‚îÄ error.html What you\u0026rsquo;ve completed Created two required HTML files Built a simple responsive layout Customized personal information placeholder Verified local testing steps Important notes Encoding: Save files as UTF-8 to avoid character issues. Filenames: Use lowercase index.html (S3 looks for this name as the default root file). If you want, I can also create a committed patch for these files or add translated versions of other workshop pages.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/5-workshop/5.3-workshop-module2/","title":"Module 2","tags":[],"description":"","content":"Upload Website to Amazon S3 Objective In this module you will:\nCreate an S3 bucket to store the website Upload the HTML files to S3 Configure S3 to host a static website Your site will get a public URL and be accessible from anywhere! Background What is S3? Amazon S3 (Simple Storage Service) is AWS\u0026rsquo;s object storage service. You can think of S3 as a huge \u0026ldquo;cloud hard drive\u0026rdquo; for storing files.\nWhat is S3 Static Website Hosting? S3 offers a feature that lets you host static websites (HTML, CSS, JS, images) without a web server. Key benefits:\nCheap (~$0.023/GB/month) No server management Automatic scaling for traffic spikes 11 nines of durability (99.999999999%) Step 1: Access the AWS Console 1.1 Sign in to AWS Open your browser and go to: https://console.aws.amazon.com Sign in with either: Root user email + password, OR IAM user (if provided by your organization) Select the region Asia Pacific (Singapore) ap-southeast-1 in the top-right corner Why Singapore? It\u0026rsquo;s geographically close to Vietnam, so it provides lower latency and reasonable pricing.\n1.2 Find the S3 service Type \u0026ldquo;S3\u0026rdquo; into the service search box in the console header Step 2: Create an S3 Bucket 2.1 Start creating the bucket In the S3 Console, click the orange Create bucket button 2. The bucket creation form includes several options 2.2 General configuration Bucket name:\nportfolio-yourname-2025 Important notes about bucket names:\nThe name must be globally unique (no other AWS account can use the same name) Use only lowercase letters, numbers and hyphens (-) No Vietnamese characters, no spaces Length between 3 and 63 characters Valid examples:\nportfolio-anhnguyen-2024 my-awesome-website-123 test-bucket-hcm AWS Region:\nAsia Pacific (Singapore) ap-southeast-1 2.3 Object ownership ACLs disabled (recommended) Explanation: ACLs are an older access control mechanism. AWS recommends using Bucket Policies for easier management.\n2.4 Block Public Access configuration IMPORTANT - READ CAREFULLY:\nBy default AWS blocks all public access for security. Because this bucket will host a public website, you need to disable those blocks for this bucket:\nUncheck all 4 options under \u0026ldquo;Block Public Access\u0026rdquo;:\nBlock all public access Block public access to buckets and objects granted through new access control lists (ACLs) Block public access to buckets and objects granted through any access control lists (ACLs) Block public access to buckets and objects granted through new public bucket or access point policies A yellow security warning will appear after you uncheck them\nSecurity warning: Only do this for a bucket that hosts a public website. Do not disable public access on buckets that store sensitive data.\n2.5 Finish Click the Create bucket button at the bottom of the page.\nStep 3: Upload Files to S3 3.1 Open the newly created bucket In the buckets list, click the name of the bucket you created You\u0026rsquo;ll land on the Objects view (file listing) 3.2 Upload files Click the orange Upload button Click Add files Select the two files: index.html and error.html from your my-portfolio folder Click Open You should see both files listed:\nüìÑ index.html üìÑ error.html 3.3 Keep default upload settings Scroll to the bottom and click Upload Wait for the progress bar to reach 100% You should see \u0026ldquo;Upload succeeded\u0026rdquo; for both files Click Close to return to the bucket view 3.5 Confirm upload You should now see the two files in the bucket:\nName Type Size Last modified index.html text/html ~8 KB Just now error.html text/html ~1 KB Just now Checkpoint: Files are uploaded to S3!\nStep 4: Enable Static Website Hosting 4.1 Open Properties tab While inside the bucket, click the Properties tab Scroll to the bottom 4.2 Find Static website hosting Locate the Static website hosting section ‚Äî it should display Disabled initially\n4.3 Enable and configure Click Edit inside the Static website hosting section Configure as shown in the screenshots Click Save changes 4.4 Save the Website Endpoint URL After saving, the Bucket website endpoint will appear. Example:\nhttp://portfolio-yourname-2025.s3-website-ap-southeast-1.amazonaws.com üìù Important: Copy this URL and save it (Notepad/Notes). The URL format is: http://[bucket-name].s3-website-[region].amazonaws.com\n‚úÖ Checkpoint: Static website hosting is enabled.\nStep 5: Configure Bucket Policy (Allow Public Read) 5.1 Why the site may still be inaccessible If you open the endpoint URL now you may get:\n403 Forbidden Access Denied This happens because objects are not yet publicly readable.\n5.2 Open the Permissions tab Click the Permissions tab Scroll to Bucket policy 5.3 Add a Bucket Policy Click Edit in the Bucket policy section Paste the following JSON into the editor: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::YOUR-BUCKET-NAME/*\u0026#34; } ] } Replace YOUR-BUCKET-NAME with your actual bucket name (e.g. portfolio-anhnguyen-2025) 5.4 What the policy does Explanation:\n\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; ‚Üí Allows access \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; ‚Üí Everyone \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34; ‚Üí Read/download objects \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::portfolio-anhnguyen-2025/*\u0026#34; Security note: This policy only grants read access; it does not allow delete, write, or upload operations.\n5.5 Save the policy Click Save changes\nCheckpoint: Bucket policy configured for public read access.\nStep 6: Test the Website 6.1 Open the website Open a new browser tab Paste the endpoint URL saved in step 4.4: http://portfolio-yourname-2025.s3-website-ap-southeast-1.amazonaws.com Press Enter 6.2 Expected result You should see the portfolio website with:\nA purple-blue gradient header Your name displayed Sections: About, Skills, Projects, Contact A footer at the bottom 6.3 Test the 404 page Open a non-existing path, e.g.:\nhttp://portfolio-yourname-2025.s3-website-ap-southeast-1.amazonaws.com/test123 You should see the 404 page with:\nA large \u0026ldquo;404\u0026rdquo; number Message \u0026ldquo;Oops! The page you\u0026rsquo;re looking for cannot be found.\u0026rdquo; A link back to the homepage 6.4 Share with others Your site is now live and accessible worldwide ‚Äî copy the URL and share it with friends for testing.\nüéâ Congratulations ‚Äî Module 2 Complete! What you accomplished: Created a globally unique S3 bucket Uploaded two HTML files to S3 Enabled Static Website Hosting Configured a public Bucket Policy Verified the site is online and tested the 404 page If you want, I can also create an English version of other workshop pages or make small edits to improve wording for documentation.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/5-workshop/5.4-workshop-module3/","title":"Module 3","tags":[],"description":"","content":"Add CloudFront - HTTPS \u0026amp; Global CDN Objective In this module you will:\nCreate a CloudFront distribution to deliver your website globally Enable free HTTPS for your website Speed up page load times by 5-10x Make your website more professional with a green lock icon in the address bar Background What is CloudFront? Amazon CloudFront is AWS\u0026rsquo;s Content Delivery Network (CDN) service.\nHow it works:\nUser in Hanoi ‚Üí CloudFront Edge (Hanoi) ‚Üí Return content instantly User in Tokyo ‚Üí CloudFront Edge (Tokyo) ‚Üí Return content instantly User in London ‚Üí CloudFront Edge (London) ‚Üí Return content instantly Instead of all users accessing S3 in Singapore, they access the CloudFront edge location nearest to them.\nWhy do you need CloudFront? 1. Free HTTPS (Security) S3 static websites only support HTTP CloudFront provides free HTTPS Green lock in browser ‚Üí Increased trust Google prioritizes HTTPS websites in search results 2. Faster load times Without CloudFront:\nUsers in Vietnam accessing S3 Singapore: ~50-100ms Users in Europe accessing S3 Singapore: ~300-500ms Users in the US accessing S3 Singapore: ~200-400ms With CloudFront:\nAll users: ~20-50ms (accessing the nearest edge location) 5-10x improvement! 3. More professional Shorter URL: d123abc.cloudfront.net vs bucket.s3-website-region.amazonaws.com Support for custom domains Advanced features (geo-blocking, custom headers, Lambda@Edge) Step 1: Access CloudFront Console 1.1 Open CloudFront From the AWS Console:\nType \u0026ldquo;CloudFront\u0026rdquo; in the service search box: 1.2 Welcome screen If this is your first time using CloudFront, you\u0026rsquo;ll see a welcome page.\nClick the orange Create distribution button.\nStep 2: Configure Origin (Content Source) Step 7: Test CloudFront 7.1 Copy Distribution Domain Name In the distributions list or in the distribution details:\nDomain name: d1234567890abcd.cloudfront.net Copy this domain.\n7.2 Access the website via HTTPS Open your browser and enter:\nhttps://d1234567890abcd.cloudfront.net Note: Remember to add https:// at the beginning!\n7.3 Expected result Website displays correctly:\nPurple-blue gradient header All content renders properly Layout is not broken Green lock in the address bar:\nhttps://d1234567890abcd.cloudfront.net Click the lock to see:\nConnection is secure Certificate is valid 7.4 Test HTTP to HTTPS redirect Try accessing with HTTP (no \u0026rsquo;s\u0026rsquo;):\nhttp://d1234567890abcd.cloudfront.net The browser will automatically redirect to:\nhttps://d1234567890abcd.cloudfront.net 7.5 Test 404 page Access a non-existent URL:\nhttps://d1234567890abcd.cloudfront.net/test-404 Your error.html page will display (large \u0026ldquo;404\u0026rdquo; with gradient background)\nUnderstanding CloudFront Better How caching works First request:\nUser ‚Üí CloudFront Edge ‚Üí (MISS) ‚Üí S3 ‚Üí CloudFront ‚Üí User ‚îî‚îÄ Save to cache Subsequent requests:\nUser ‚Üí CloudFront Edge ‚Üí (HIT) ‚Üí User (Serve from cache instantly) Cache duration Default: 24 hours (using CachingOptimized policy)\nAfter 24 hours, CloudFront checks S3 to see if files have changed.\nWhat is Invalidation? When you update files on S3, CloudFront still serves the old cached version.\nHow to force CloudFront to update immediately:\nGo to CloudFront Console ‚Üí Select your distribution Go to \u0026ldquo;Invalidations\u0026rdquo; tab ‚Üí Click \u0026ldquo;Create invalidation\u0026rdquo; Object paths: /* (invalidate all) Click \u0026ldquo;Create invalidation\u0026rdquo; Wait 1-2 minutes ‚Üí Fresh cache! Note: The Free Tier includes 1,000 free invalidation paths per month.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material MON - Form a group and connect with group members - Spend time reading and memorizing the regulations before going to the internship office - Watch introduction videos about AWS on YouTube: + What is AWS and Cloud Computing? + AWS Global Infrastructure + Cost Optimization on AWS + \u0026hellip; 08/09/2025 08/09/2025 https://www.youtube.com/@AWSStudyGroup TUE - Create an AWS Free Tier account \u0026amp; set up MFA following the provided documentation - Learn how to use AWS Console: + Create Admin group and Admin user + Set up security for the Admin user + Configure region and initialize dashboard widgets + Learn and clearly understand AWS support plans 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/ https://www.youtube.com/@AWSStudyGroup WED - Learn about EC2: + What is EC2? + AMI + EBS Volume + Snapshot + Instance + Security Group (Access restriction) 10/09/2025 10/09/2025 https://www.youtube.com/watch?v=6PqZVGoeEEA\u0026list=PLgT9f4ZU9cncuJ5ACqQxook4yKO1xuZP6\u0026index=3 THU - Review theory on Subnet, Route Table, Internet Gateway, NAT Gateway - Practice: + Create VPC, Subnet, Internet Gateway, Route Table, and Security Group + Launch EC2 with instance type: t3.micro + Connect to the EC2 Private instance + Create a NAT Gateway 11/09/2025 11/09/2025 https://000003.awsstudygroup.com/vi/4-createec2server/ https://www.youtube.com/watch?v=6PqZVGoeEEA\u0026list=PLgT9f4ZU9cncuJ5ACqQxook4yKO1xuZP6\u0026index=4 FRI - Learn about Site-to-Site VPN - Practice: + Create VPC for VPN + Create VPG and Customer Gateway + Establish a VPN connection + Configure and Customize Customer Gateway \u0026amp; AWS VPN Tunnel 12/09/2025 12/09/2025 https://000003.awsstudygroup.com/vi/5-vpnsitetosite/ Week 1 Achievements: General Tasks Formed a team and connected with members. Understood and memorized internship regulations. AWS Fundamentals Gained knowledge via introduction videos: Cloud \u0026amp; AWS concepts AWS Global Infrastructure Cost Optimization on AWS AWS Account Management Successfully created an AWS Free Tier account and enabled MFA. Used AWS Console to: Create Admin group and Admin user Configure Admin user security Set region and dashboard widgets Learn about AWS Support Plans EC2 Acquired knowledge of EC2, AMI, EBS, Snapshot, Instance, Security Group. Basic Networking Infrastructure Practiced: Creating VPC, Subnet, Internet Gateway, Route Table, Security Group Launching EC2 (t3.micro) Connecting to Private EC2 instance Creating a NAT Gateway VPN Learned and practiced configuring Site-to-Site VPN: Created VPC for VPN Created VPG and Customer Gateway Established VPN Tunnel Customized connection between Customer Gateway \u0026amp; AWS VPN Tunnel "},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material MON - 09/08/2025 09/08/2025 \u0026lt;\u0026gt; TUE - 09/09/2025 09/09/2025 \u0026lt;\u0026gt; WED - 09/10/2025 09/10/2025 \u0026lt;\u0026gt; THU - 09/11/2025 09/11/2025 \u0026lt;\u0026gt; FRI - 09/12/2025 09/12/2025 \u0026lt;\u0026gt; SAT - 09/13/2025 09/13/2025 \u0026lt;\u0026gt; SUN - 09/14/2025 09/14/2025 \u0026lt;\u0026gt; Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 3 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material MON - 09/08/2025 09/08/2025 \u0026lt;\u0026gt; TUE - 09/09/2025 09/09/2025 \u0026lt;\u0026gt; WED - 09/10/2025 09/10/2025 \u0026lt;\u0026gt; THU - 09/11/2025 09/11/2025 \u0026lt;\u0026gt; FRI - 09/12/2025 09/12/2025 \u0026lt;\u0026gt; SAT - 09/13/2025 09/13/2025 \u0026lt;\u0026gt; SUN - 09/14/2025 09/14/2025 \u0026lt;\u0026gt; Week 3 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material MON - 09/08/2025 09/08/2025 \u0026lt;\u0026gt; TUE - 09/09/2025 09/09/2025 \u0026lt;\u0026gt; WED - 09/10/2025 09/10/2025 \u0026lt;\u0026gt; THU - 09/11/2025 09/11/2025 \u0026lt;\u0026gt; FRI - 09/12/2025 09/12/2025 \u0026lt;\u0026gt; SAT - 09/13/2025 09/13/2025 \u0026lt;\u0026gt; SUN - 09/14/2025 09/14/2025 \u0026lt;\u0026gt; Week 4 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 5 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Aurora Time Unified AWS Serverless Solution for Personal Time Management 1. Executive Summary 67890 This proposal presents the implementation plan for Aurora Time, a time management application on the AWS platform, simple and focused on scheduling features, to address the complexity and operational costs of current solutions. Aurora Time will leverage AWS serverless and managed services to ensure high scalability, reliability, and cost optimization, delivering rapid return on investment (ROI) through reduced infrastructure management costs.\n2. Problem Statement Current Problem Individuals struggle to manage daily commitments because schedules are scattered (notes, phones) leading to confusion and missed tasks. Current tools are often too complex, overloaded with work features, and unsuitable for the need for quick and simple scheduling in personal life. Aurora Time solves this by providing a centralized, minimalist, and intuitive platform to easily track habits and important milestones.\nSolution The platform uses Amazon S3 combined with Amazon CloudFront to store and distribute web applications, using AWS Amplify for rapid development and deployment. Amazon API Gateway and AWS Lambda serve as the Backend processing layer to handle event CRUD requests. Data is stored in Amazon DynamoDB to ensure fast access speed and low latency. Amazon Cognito ensures secure authentication and authorization for each individual user. Amazon EventBridge and Amazon SES are used to trigger and send scheduled reminder notifications. Similar to other calendar applications, users can create and edit personal schedules, but this platform operates at a more minimalist scale and serves the purpose of daily personal time management. Key features include intuitive scheduling interface, customizable reminders, and low operational costs.\nBenefits and Return on Investment (ROI) The Aurora Time solution creates a solid foundation for individual users to centralize all schedules while providing a cost-saving Serverless architecture model for easy feature expansion. The platform reduces schedule fragmentation and minimizes missed important commitments through a centralized, simple reminder system, simplifying personal time management and improving work/life balance.\nEstimated monthly infrastructure costs of $16 - $50 USD, totaling approximately $192 - $600 USD for 12 months (depending on number of users and requests). All development components are based on Serverless, incurring no hardware purchases or 24/7 virtual server rental costs. The payback period is estimated at under 6 months thanks to significant time savings in searching and manually arranging schedules, and the extremely low operational costs of the AWS Serverless architecture.\n3. Solution Architecture The platform applies AWS Serverless architecture to manage personal schedule and event data, with the capability to easily scale from a single user to millions of individual users. API requests are received through Amazon API Gateway and processed by AWS Lambda, while data is stored in Amazon DynamoDB to ensure fast query speed and low latency. Amazon EventBridge handles reminder scheduling logic, triggers Lambda, and sends notifications. AWS Amplify provides an intuitive web/mobile interface, secured by Amazon Cognito to safely manage access permissions for each user.\nAWS Services Used AWS Lambda: Processes business logic for event CRUD operations and triggers scheduled reminder tasks. Amazon API Gateway: Provides secure RESTful API interface for communication with web applications. Amazon DynamoDB: Stores event data, schedules, and user information. Amazon S3 and CloudFront: Stores and distributes static content of Frontend applications. Amazon EventBridge: Schedules and triggers automatic reminder events at user-defined times. Amazon SES: Sends customized reminder notifications via email (SES). AWS Amplify: Stores and provides intuitive web interface. Amazon Cognito: Manages access permissions and secure authentication for individual users. Component Design User Interface: AWS Amplify hosts web applications (planned to use React) providing intuitive scheduling interface, schedule viewing, and reminder settings. User Authentication: Amazon Cognito manages individual user accounts, issuing secure authorization tokens for Backend API access. API Request Reception: Amazon API Gateway receives and routes authenticated requests (e.g., create event, query schedule) to corresponding Lambda functions. Backend Logic Processing: AWS Lambda processes and executes business logic. Meanwhile, other Lambda functions are triggered by EventBridge to send reminders. Data Storage: Amazon DynamoDB stores primary data (Schedules, Events, User Information) in NoSQL format, ensuring fast access and flexibility. Reminder Scheduling: Amazon EventBridge schedules and triggers events at user-defined times, ensuring reminder features operate automatically. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases\nThe project consists of 2 parts ‚Äî Backend Serverless setup and Frontend User Interface building ‚Äî each part goes through 4 phases:\nResearch and Architecture Design: In-depth research on DynamoDB Data Modeling for schedules and design of verified AWS Serverless architecture (API Gateway, Lambda, EventBridge). (Timeline: Month 1) Cost Calculation and Feasibility Check: Use AWS Pricing Calculator to estimate actual Serverless operational costs and verify feasibility of authentication (Cognito) and storage (DynamoDB) flows. (Timeline: Month 1) Architecture Adjustment for Cost/Solution Optimization: Fine-tune Lambda parameters (e.g., memory, timeout) and DynamoDB (e.g., RCU/WCU) to ensure highest cost efficiency and best performance for individual users. (Timeline: Month 2) Development, Testing, Deployment: Program Lambda functions, set up CI/CD Pipeline with CodePipeline/CodeBuild/CloudFormation, and develop Frontend applications (React). Then conduct Beta testing and go live. (Timeline: Month 2-3) Technical Requirements\n1. Backend Requirements (Serverless)\nCore Services: In-depth knowledge of AWS Lambda (Node.js), Amazon DynamoDB, Amazon API Gateway, and Amazon Cognito. Event Management: Proficiency in Amazon EventBridge to schedule and trigger Lambda functions sending reminders via Amazon SES/SNS. 2. Frontend Requirements\nInterface: Practical knowledge of AWS Amplify to host React applications and connect to API Gateway Optimization: Leverage React\u0026rsquo;s processing capabilities to reduce load on Lambda functions 5. Roadmap \u0026amp; Deployment Milestones Internship (Month 1‚Äì3): Month 1: Learn AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Deploy, test, and go live. Post-Deployment: Further research over 1 year. 6. Budget Estimation Infrastructure Costs\nAWS Amplify: $0.35/month S3 Standard: $0.05/month CloudFront: $1.70/month API Gateway: $0.11/month AWS Lambda: $0.00 (free) /month DynamoDB: $0.11/month Amazon Cognito: $0.00/month Amazon SES (email): $0.05/month EventBridge: $0.10/month CloudWatch Logs: $0.10/month CI/CD Pipeline + Build: $0.00 (free) /month Total: $2.57/month, $30.84/12 months\n7. Risk Assessment Risk Matrix\nNetwork disconnection/High latency: Medium impact, medium probability. DynamoDB design error: High impact, medium probability. Serverless costs exceed budget: Medium impact, low probability. Reminder system failure: High impact, low probability. Mitigation Strategies\nNetwork disconnection/High latency: Optimize Frontend (stored on S3/CloudFront) to ensure fast loading speeds, use Client-side Caching so users can still view recent schedules when network is lost. DynamoDB design error: Conduct strict Proof of Concept (POC) and Load Testing for data models. Use optimized Global Secondary Index (GSI) to avoid querying entire table and minimize RCU/WCU costs. Serverless costs exceed budget: Set up AWS Budgets with proactive alerts (Email/SNS) when spending approaches threshold. Regularly check AWS Cost Explorer and optimize Lambda resources. Reminder system failure: Closely monitor EventBridge and Lambda reminder processing through CloudWatch Alarms to detect errors immediately. Implement Retry Logic for critical Lambda functions. Contingency Plan\nAWS Incident: Since the application is only for individuals, if AWS encounters an incident, the system will be configured to automatically Rollback to the latest code version (via CodePipeline) or re-deploy configuration (via CloudFormation). Data Loss: Set up DynamoDB Backup and Restore at regular intervals to quickly recover event data in case of system failure or user error. Cost Issues: Use CloudFormation to restore resource configuration (such as RCU/WCU) to proven low-cost state. 8. Expected Results User Experience Improvement: Provide intuitive scheduling interface and real-time notifications (Real-time notifications) replacing manual note-taking and schedule tracking processes. The system is designed for easy scaling, serving from a single user to millions of individual users.\nLong-term Value and Reusability: Create a solid Serverless technical platform that can maintain operation at extremely low costs for many years. This architecture can be reused for personal application projects or other expansion features in the future (e.g., habit tracking, simple personal task management).\nSuccessful Deployment: Successfully deploy the entire approved Serverless architecture, including automated CI/CD and core features of Aurora Time within the planned timeframe.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://Kienht7704.github.io/Internship-Report/5-workshop/5.6-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"Clean Up Resources First ‚Äî congratulations on completing this lab! Now let\u0026rsquo;s clean up the resources you created.\nDelete the S3 bucket Disable / Delete the CloudFront distribution After disabling the distribution, AWS will allow you to delete the CloudFront distribution.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://Kienht7704.github.io/Internship-Report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://Kienht7704.github.io/Internship-Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://Kienht7704.github.io/Internship-Report/tags/","title":"Tags","tags":[],"description":"","content":""}]