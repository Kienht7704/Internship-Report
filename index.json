[{"uri":"https://Kienht7704.github.io/Internship-Report/4-eventparticipated/4.1-event1/","title":"AWS Cloud Day Vietnam - AI Edition 2025","tags":[],"description":"","content":"AWS Cloud Day Vietnam - AI Edition 2025 - Date: September 18, 2025 - Location: 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nEvent Overview A pivotal gathering for Vietnam\u0026rsquo;s tech and business communities, focused on accelerating digital transformation through the convergence of Cloud Computing and Artificial Intelligence.\nKey Objectives:\nDemocratize Generative AI: Move GenAI from concept to practical, context-aware enterprise applications. Align Business \u0026amp; IT: Bridge the gap between business goals and IT, especially in Financial Services. Accelerate Modernization: Provide industry-specific roadmaps for migration and cloud-native development. Strengthen Security: Promote a \u0026ldquo;security by design\u0026rdquo; mindset across the application lifecycle. Key Takeaways \u0026amp; Learnings Data is the Differentiator: A comprehensive data strategy is a prerequisite for successful Generative AI. Modernization is a Continuous Journey: The goal is not just to migrate but to \u0026ldquo;Migrate to Operate\u0026rdquo; and innovate continuously. Business-Led Technology: Technology initiatives must be driven by clear business outcomes. Security is Everyone\u0026rsquo;s Responsibility: Security must be integrated from the first line of code. Application to Work Audit Data Readiness: Assess our current data strategy to ensure it can support future GenAI initiatives. Pilot GenAI in DevOps: Experiment with AI-driven code generation and automated testing to improve development velocity. Benchmark Modernization Efforts: Analyze case studies from Honda Vietnam (SAP migration) and Masterise Group (VMware migration) to refine our own modernization roadmaps. Implement \u0026ldquo;Security at Scale\u0026rdquo;: Integrate security tools and best practices throughout the entire development lifecycle. Event Photos "},{"uri":"https://Kienht7704.github.io/Internship-Report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"AWS Powers Breakthrough Gaming Experiences at devcom and gamescom 2025 Author: Jeff Harris and David Holladay Published: August 13, 2025 Source: https://aws.amazon.com/blogs/gametech/aws-powers-breakthrough-gaming-experiences-at-devcom-and-gamescom-2025/ Amazon Web Services (AWS) will be coming to Cologne, Germany this month to participate in two major gaming industry events: the devcom Developer Conference and gamescom, running August 17-19 and August 20-24 respectively at Koelnmesse. The AWS for Games team will share advances that help developers build, operate, and grow their games while connecting with fans and industry experts from around the world.\nAWS will launch the week of events with the Celebrating Women in Games reception and networking event on Monday, August 18 at 6:00 PM at K√∂lnSKY, in partnership with Amazon Games, EPAM Systems, and Women in Games International (WIGI). Marking WIGI\u0026rsquo;s 20th anniversary, the event will celebrate and recognize achievements in diversity and inclusion. Be sure to register today to make sure you don\u0026rsquo;t miss a panel discussion with successful female leaders‚Äîa key part of the program.\nBeyond that, AWS has prepared a robust slate of activities for devcom and gamescom attendees to learn, connect, and get inspired.\nAWS Recognized as Leading Cloud Platform for Gaming The newly released 2025 Omdia Market Radar has positioned AWS as the leading cloud platform provider for gaming. The report evaluates major cloud providers serving the gaming industry, analyzing their capabilities across game development, operations, and emerging technologies. According to Omdia\u0026rsquo;s analysis, AWS leads the cloud services market for gaming, particularly outside China. AWS has held this top position for three consecutive years, with standout differentiators such as advanced game development tools, proven scalability, and continuous innovation, especially in AI.\nFigure 1: Omdia\u0026rsquo;s heat map of cloud platforms for gaming\nWhat You Can Expect \u0026amp; Where to Meet AWS for Games During devcom, attendees can meet with AWS for Games industry experts and partners, explore new solutions and updates at the exhibit area (booth A1) throughout opening hours, and attend several customer-led panel discussions. Some highlights include:\nLive Demo Experiences: Containers on Amazon GameLift servers\nGame streaming with Amazon GameLift Streams\nHarnessing AI power for QA with partner Razer Inc\nContinuous localization for games, powered by AI with partner Phrase\nDeep understanding of player journeys with AppsFlyer\u0026rsquo;s devtodev\nAI-integrated gaming solutions with partner Globant\nFeatured Workshop Sessions: During Monday and Tuesday, AWS for Games will host four panel discussions, including:\nFeatured Workshop Sessions - Building Game Streaming Experiences with Amazon GameLift Streams Monday, 8/18 | 11:45‚Äì12:15 | Stage 3, Confex 1 Floor\nDiscover how to leverage the power of pixel streaming with Amazon GameLift Streams, from key deployment considerations to security measures and performance optimization. This discussion introduces practical approaches for building cloud gaming solutions that deliver high-fidelity, immersive gameplay experiences to players on any device without downloads. Use cases include: direct-to-player game streaming, efficient internal playtesting, and playable ads to attract new players.\n- Monster Hunter Wilds Network Architecture Supports Over 1 Million Concurrent Players Monday, 8/18 | 13:15‚Äì14:25 | Stage 15, Confex 2 Floor\nMonster Hunter Wilds from Capcom is a cross-platform AAA title designed to handle massive player volumes from day one. This panel shares how the development team architected the game network to support enormous global player traffic‚Äîincluding insights into technology choices, challenges faced, and innovative infrastructure solutions built from the ground up for optimal performance.\n- Global Growth Starts Early: Game Localization Workflow for Developers (with Phrase) Tuesday, 8/19 | 11:45‚Äì12:15 | Stage 3, Confex 1 Floor\nGlobal players expect authentic, high-quality experiences in their own language. However, many studios handle localization late in development, leading to release delays, reduced quality, and increased rework costs. This discussion shows how to plan localization early, automate processes, and use integrated cloud tools to help developers efficiently scale across multiple markets and platforms. The session also shares practical strategies from successful studios, demonstrating how intelligent localization workflows can shorten development timelines, reduce crunch, and drive sustainable global growth.\n- AI ‚Äì Your Development Ally: Build Smarter, Scale Faster (with Razer) Tuesday, 8/19 | 13:15‚Äì13:45 | Stage 3, Confex 1 Floor\nGame quality assurance (QA) is often costly and time-consuming, extending development cycles and increasing operational costs. In this intimate conversation, Razer and AWS will present how Razer QA CO-AI can transform workflows by automatically detecting bugs, crashes, and performance issues during gameplay. As a result, developers can build higher-quality games faster and more efficiently.\nNew Solutions for Game Development Some new products and guidance from AWS for Games include:\nGuidance for Game Analytics Pipelines on AWS: Updated architecture and code samples with comprehensive new features. Includes Terraform support, Apache Iceberg tables, Amazon Redshift Serverless integration, and Amazon Managed Service for Apache Flink. These improvements help game studios collect, store, and analyze game data faster and more flexibly in their preferred way. They provide powerful real-time insights while keeping management costs low and scalability high.\nHydrolix for AWS: Now available on AWS Marketplace. This is a SaaS observability solution for game studios, providing real-time observability and reasonable costs. During load testing, Hydrolix for AWS provides instant visibility into log data from multi-CDN, origin, and AWS edge services to identify performance bottlenecks and trace the root cause and trends across previous events.\nGuidance for Building Perforce Helix Core on AWS: Now includes sample code as part of the Cloud Game Development Toolkit. This guidance shows how to install and configure Perforce P4 (Helix Core), a popular version control tool in game development, on AWS. By following this guidance, game developers can deploy Perforce P4 on AWS according to best practices while keeping costs low.\nGuidance for Intelligent Identification of 2D/3D Assets on AWS: Features more cost-optimized architecture and new code samples to help game developers automatically identify and manage assets using AI/ML technology. This solution uses Amazon Rekognition to automatically analyze and tag assets as they are uploaded, helping teams save hundreds of hours of manual work while providing distributed creative teams with secure access and global asset search capabilities. These improvements help studios streamline asset workflows, enhance collaboration, and accelerate time-to-market through more efficient creative pipelines.\nDuring gamescom, AWS for Games experts will be available to meet Wednesday through Friday during exhibit hours at the devcom B2B lounge in Hall 4.1 C011.\nConnect with AWS Visit the AWS for Games booth (A1) at devcom to see the latest services and solutions firsthand, while connecting with AWS gaming experts. You can also schedule time to meet with the AWS team through the gamescom biz website or event app.\nWe look forward to meeting you at devcom and gamescom to explore cutting-edge technology together.\nAbout the Authors "},{"uri":"https://Kienht7704.github.io/Internship-Report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Flexibility to Framework: Building MCP Servers with Controlled Tool Orchestration Author: Kevon Mayers Published: August 13, 2025 Source: Flexibility to Framework: Building MCP Servers with Controlled Tool Orchestration | AWS DevOps \u0026amp; Developer Productivity Blog\nMCP (Model Context Protocol) is a protocol designed to standardize interactions with generative AI models, making it easier to build and operate AI applications. The protocol provides a consistent way to pass context between an application and a model regardless of where or how the model is deployed. MCP helps close the gap between model deployment and application development by offering a unified interface for model interactions.\nWhile MCP provides flexibility in tool choice, there are important challenges when you need to enforce an order of tool usage. In this article, I describe how I designed this capability and implemented it in the AWS Cloud Control API (CCAPI) MCP server.\nThe challenge ‚Äî enforcing tool order in MCP When you think about MCP, you naturally imagine flexibility. One of the primary reasons to use an MCP server is to let a large language model (LLM), via an agent, access a set of tools such as reading from a database, sending an email, or other helper functions. However, the MCP framework does not provide an intrinsic mechanism to force the order in which tools are called.\nFor example: consider two tools, fetch_weather_data() and send_email(). You want to ensure the email includes current weather ‚Äî meaning fetch_weather_data() must run before send_email(). Or consider getOrderId() and getOrderDetail(), where you must obtain the OrderId before fetching detailed order information. Because MCP currently lacks a way to define tool-order preferences, enforcing such sequential relationships is difficult.\nMCP tools are designed as independent functions the LLM can call when needed. There is no concept of a workflow or sequence inside the MCP framework itself. Each tool invocation is treated as a separate task with no built-in knowledge of what happened before or what will happen next. As a result, by default an LLM may call tools in any order, ignoring the logical sequence you might expect.\nWhile LLMs are excellent at flexible decision-making, certain use cases like infrastructure management require strict ordering. This raises the question for MCP server developers: how do you preserve LLM flexibility while enforcing the required order of critical tools?\nWhen you think about Infrastructure as Code (IaC), you expect repeatability, consistency, versioning, and CI/CD integration. CI/CD pipelines follow a defined sequence:\nPull request is created CI/CD pipeline triggers A series of steps run (linting, security checks, unit tests, end-to-end tests, etc.) If any step fails, the pipeline stops The non-deterministic nature of AI complicates this: generative AI is not deterministic ‚Äî the same prompt might not always produce the same result. If outputs deviate too much from expectations, that\u0026rsquo;s considered a hallucination. So how do you guide an LLM to do what you want? Let\u0026rsquo;s look at how the CCAPI MCP server addresses this.\nUnderstanding tool discovery and initialization in MCP Before diving into the solution, it\u0026rsquo;s important to understand how an MCP server communicates available tools to AI agents. During initialization, MCP defines lifecycle stages where capabilities and tools are discovered.\nThe MCP context model defines a structured lifecycle for the client-server connection that negotiates capabilities and manages state. The stages include:\nInitialization: negotiate capabilities and protocol versions Operation: normal protocol communication Shutdown: gracefully close the connection Initialization establishes compatibility and shares implementation details. This is when an AI agent learns about available tools via schema definitions and receives usage guidance. Initialization is critical because tools are discovered at this stage. The client sends protocol version information, capability declarations, and deployment details during initialization.\nFor example, a tool like an Amazon Q CLI receives information about the MCP server version, available tools, and usage guidance via schemas sent by the server.\n(See the MCP lifecycle documentation for more details: https://modelcontextprotocol.io/specification/2025-06-18/basic/lifecycle.)\nSolution ‚Äî token-based tool orchestration: a new pattern for MCP agents MCP faces a challenge: tools cannot directly talk to each other to enforce an execution order. The CCAPI MCP server solves this by using a token messenger pattern, where the server issues and controls verification tokens, and the AI agent (the MCP client) passes those tokens between tool invocations.\nKey implementation details Function enhancement\nA @mcp.tool() decorator turns each function into an enhanced entity. It wraps the function with a schema that defines required inputs and validation rules while preserving the docstring. Each function explicitly exposes its requirements and raises clear errors if dependencies are not met.\nDependency discovery\nDuring MCP initialization the AI agent receives a full map of tools and their schemas from the MCP server. The LLM (acting inside the agent) uses these schemas to infer dependencies through parameter descriptions and required inputs.\nFor example, if a tool requires a parameter described as \u0026ldquo;Result from get_aws_session_info()\u0026rdquo; and declares security_scan_token as a required input, the LLM will understand that it needs valid tokens before continuing. The combination of textual descriptions and clear required inputs enables the agent to execute a chain like get_aws_session_info() ‚Üí generate_infrastructure_code() ‚Üí run_checkov() ‚Üí create_resource().\nToken validation control\nThe server issues and controls all workflow tokens via a server-side store (_workflow_store). Each tool in the workflow emits a security token that is stored on the server with associated metadata.\nThe AI agent holds tokens in its chat context and passes them between tool calls. Each token used by the agent must be validated against the server-side store. Tokens are short-lived, kept in memory, actively managed, and removed after use to preserve freshness. Any leftover tokens are cleared when the server process shuts down or restarts.\nIf a token does not exist in the store (because it is invalid or was already consumed), the operation immediately fails with an error. This applies to all token types and ensures that the agent cannot fabricate or modify tokens.\nAs the workflow progresses, a tool consumes existing tokens and produces new tokens. For example, when explain() receives a properties_token, it validates that the token exists and matches the _workflow_store, then consumes it and emits a new explained_properties_token. This approach forms a cryptographic-like chain of operations that enforces workflow order (generate ‚Üí scan ‚Üí create) with server-side verification at each step.\nThe result is a predictable workflow system with strong security controls ‚Äî tokens must be issued by the server and validated against the server store at every step ‚Äî which enforces tool ordering and data integrity in infrastructure operations. This approach enables reliable workflow execution within the constraints of the current FastMCP framework.\nWhile explicit dependency declarations like @mcp.tool(depends_on=[\u0026quot;run_checkov\u0026quot;]) (as discussed in a GitHub issue) may be ideal in future versions, the token approach combining descriptive parameter names and explicit validation currently provides a stable ordering mechanism the LLM will follow.\nPotential limitations and mitigations Session management\nWhen an agent session terminates or refreshes, any in-progress workflows must restart. This is by design ‚Äî tokens are short-lived and bound to a specific workflow chain. AWS credentials naturally expire after a few hours as part of standard security policies, which provides a natural boundary for workflow sessions.\nConcurrent workflows\nEach agent interaction is isolated, which is appropriate to maintain security boundaries between workflows. While this means each session starts fresh, it guarantees clear separation between independent infrastructure operations.\nImplementation options\nOrganizations that require long-lived workflow state can use a traditional database to persist session state across restarts. However, because tokens are designed as short-lived security controls, many deployments can rely on in-memory storage with the natural session boundary.\nThe token messenger pattern provides a robust foundation for secure workflow coordination, using ephemeral tokens to guarantee tool ordering and data integrity when operating on infrastructure.\nThe future of MCP While the solution works, it also suggests future directions for MCP. I\u0026rsquo;ve seen multiple updates to the framework recently, and it\u0026rsquo;s encouraging to see community activity. With agentic AI overall, there\u0026rsquo;s evidence that platforms can become more deterministic (as emphasized by \u0026ldquo;hooks\u0026rdquo; in Claude Code). According to the docs, ‚ÄúHooks provide deterministic control over Claude Code behavior, ensuring some actions always occur rather than relying on the LLM to choose.‚Äù For IaC and other deterministic technologies we want to pair with AI, that determinism is essential for enterprise adoption.\nConclusion The Model Context Protocol (MCP) journey and the emerging space of AI-driven infrastructure management continue to evolve, bringing both opportunities and challenges to cloud computing and AI. Current approaches like prompt loading and parameter-driven dependencies have helped address early ordering and security concerns, showing MCP can be used effectively in enterprise applications.\nWhile the present implementation using workflow tokens and validation checks provides a workable solution, we continue to explore ways to enhance the protocol. If you‚Äôre interested in contributing to MCP‚Äôs evolution, check the dependency management improvement proposals in the modelcontextprotocol GitHub organization and the FastMCP repository.\nIf you want to explore the AWS MCP Cloud Control API server mentioned in the article, see the documentation and GitHub repos. If you‚Äôd like hands-on practice with it and other MCP servers, try the linked AWS workshop. Happy coding!\nAbout the author "},{"uri":"https://Kienht7704.github.io/Internship-Report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Introducing AWS Cloud Control API MCP Server: Natural Language Infrastructure Management on AWS Author: Kevon Mayers and Brian Terry Published: August 13, 2025 Source: Introducing AWS Cloud Control API MCP Server: Natural Language Infrastructure Management on AWS | AWS DevOps \u0026amp; Developer Productivity Blog\nToday we‚Äôre announcing the AWS Cloud Control API (CCAPI) MCP Server. This MCP server transforms AWS infrastructure management by allowing developers to create, read, update, delete, and list resources using natural language commands. The project is part of awslabs/mcp ‚Äî an experimental effort to bridge conversational intent and infrastructure operations on AWS.\nThe CCAPI MCP Server is powered by the AWS Cloud Control API ‚Äî a standardized API that performs CRUDL (Create/Read/Update/Delete/List) operations across AWS and third-party resources through a single access point.\nKey features Uses AWS Cloud Control API to perform CRUDL operations for over 1,200 AWS resource types\nEnables LLM-based agents and developers to manage infrastructure via natural language\nOptionally exports Infrastructure as Code (IaC) templates for created resources so you can integrate with existing CI/CD pipelines\nIntegrates with the AWS Pricing API to provide cost estimates for the infrastructure it will create\nApplies security best practices automatically via Checkov\nWhy use the CCAPI MCP Server? Simpler infrastructure management: no more wrestling with complex config templates or long manuals\nIncreased developer productivity: focus on intent, not the exact configuration syntax\nLower onboarding friction: new team members can perform common infra tasks with natural language\nLLM integration: a natural companion for AI-assisted development workflows\nThe MCP Server lets developers manage cloud resources with conversational commands such as:\n‚ÄúCan you create a new S3 bucket for me?‚Äù ‚ÄúList all my EC2 instances and tell me which ones are not t2.large‚Äù This reduces configuration overhead and makes intent-to-infrastructure mappings more direct.\nHow to create and manage cloud resources Prerequisites A package manager (pip) installed\nPython 3.x\nAWS credentials configured with appropriate permissions. The MCP server supports multiple ways to provide credentials ‚Äî see the MCP documentation for details. We recommend using dynamic credentials (for example, via SSO). For AWS credential configuration guidance, consult the AWS CLI documentation.\nAn MCP host application that can run both MCP clients and MCP servers (for example: Amazon Q Developer, Claude Desktop, Cursor, etc.). To follow the examples in this post, install Amazon Q Developer for CLI as described in its installation guide.\nIntegration with developer tools To start using the CCAPI MCP Server, configure the server settings (typically in a file named mcp.json). This post focuses on using the CCAPI MCP Server with Amazon Q Developer. Note that other MCP host apps may use different locations for their MCP config files.\nConfiguration locations include:\nGlobal configuration: ~/.aws/amazon/mcp.json ‚Äî applies to all workspaces Workspace configuration: .amazonq/mcp.json ‚Äî applies to the current workspace See the Amazon Q Developer User Guide for more details.\nConfiguration file structure The MCP config uses JSON. Example configuration screenshots are shown below.\nExample mcp.json for the CCAPI MCP Server:\nImportant configuration notes Ensure you configure AWS credentials properly for the MCP server. The server uses those credentials when calling Cloud Control API to perform CRUDL operations in your AWS account. The server supports various credential sources such as AWS profiles, environment variables, SSO tokens, etc. See aws_client.py in the project for examples and refer to named profile documentation for more details.\nRead-only mode If you want the MCP server to avoid making mutating actions (Create/Update/Delete), run it with the --readonly flag.\nSecurity considerations Ensure the IAM credentials used have the permissions required by the Cloud Control API (List, Get, Create, Update, Delete). Consult the AWS CCAPI docs for details. Follow the principle of least privilege when configuring IAM permissions. Enable AWS CloudTrail to record and audit operations. Consider running the server in read-only mode (--readonly) for safer operation when appropriate. Example: Create an S3 bucket encrypted with KMS Important: Ensure you‚Äôve satisfied the prerequisites before running the example.\nOnce mcp.json is configured correctly, try the example. In your terminal, run q chat to start Amazon Q in the CLI.\nThat command starts background MCP servers, allowing Q Chat to use them even while startup continues. If the MCP servers are still loading, your prompts may be processed without using MCP servers. To check server status, run /mcp.\nWhen servers are ready, try a sample prompt. Tell Amazon Q: Create an S3 bucket with versioning and encrypt it using a new KMS key.\nAmazon Q will automatically perform these steps:\nRead your current environment variables Use them to retrieve the current AWS session info Generate infrastructure code describing the requested resources Explain the generated code Run a security scan on the generated code (if enabled) Explain the scan results Validate the configuration against Cloud Control API schemas (based on CloudFormation Resource Provider Schemas) and IAM policies Create resources directly via Cloud Control API Note: CloudFormation schemas are used for resource structure reference only ‚Äî CCAPI MCP Server uses Cloud Control API for resource management rather than CloudFormation itself.\nBefore taking destructive actions, Amazon Q will summarize the steps and request confirmation (type y to proceed when prompted).\nNext, Amazon Q may call get_aws_session_info() to fetch session details according to values in your MCP config (e.g., ~/.aws/amazon/mcp.json) and environment variables.\nThe tool will then display the AWS account ID and region it will use, generate infrastructure code for the KMS key (and related properties), and send the properties to Cloud Control API for validation and Checkov scanning. The KMS key will be configured with recommended security practices and a key policy constrained to the current account.\nAfter the code is generated, Amazon Q will use explain() to describe the generated infrastructure code. The MCP server automatically adds default tags to managed resources: MANAGED_BY, MCP_SERVER_SOURCE_CODE, MCP_SERVER_VERSION. These tags help you identify resources managed by the MCP Server; you can customize or disable them, but keeping identifying tags is recommended for visibility.\nAmazon Q will then run run_checkov() to perform a security scan on the generated code (this runs automatically if SECURITY_SCANNING is enabled in your server config).\nIf Checkov finds issues, Amazon Q will explain the findings and suggest remediations. Passing checks proceed automatically with a short summary; request more detail if needed.\nNext, Amazon Q uses create_resource() to create resources via Cloud Control API, then polls get_resource_request_status() using the request token to track progress.\nAmazon Q will continue using CCAPI MCP Server tools until the S3 bucket and KMS key are created, then produce a summary of the workflow.\nTry asking Amazon Q to perform a potentially risky change (for example, make an S3 bucket publicly accessible). Amazon Q will warn that the change violates best practices, explain why, and request confirmation before proceeding.\nThe CCAPI MCP Server integrates with AWS Pricing API so you can request cost estimates for resources it plans to create.\nYou can also ask Amazon Q to generate a CloudFormation template of the resources it created so you can save a backup or redeploy later. The create_template() tool has these defaults:\nOutputs YAML by default (JSON option available) Sets DeletionPolicy = RETAIN Sets UpdateReplacePolicy = RETAIN Allows optional parameters such as template ID, file location, and region Refer to the tool‚Äôs source code for more details.\nTry another risky operation, like deleting all resources in an account ‚Äî the security checks will block the attempt and propose safer alternatives.\nFinally, ask Amazon Q to delete the resources it previously created. The toolchain will:\nUse get_resource() to find the resources it created Use explain() to describe the planned changes Use delete_resource() to remove them After successful deletion, Amazon Q shows a final summary.\nStarter prompts Sample prompts ‚ÄúCreate a VPC with private and public subnets‚Äù ‚ÄúList all my EC2 instances‚Äù ‚ÄúCreate a serverless API for my application‚Äù ‚ÄúSet up a load-balanced web application‚Äù Capabilities Provision a full networking environment with private and public subnets List all running EC2 instances in your account Deploy an API Gateway integrated with Lambda (serverless) Create a load-balanced web application (ALB) with a target group and instances Conclusion The AWS Cloud Control API MCP Server is an important step forward for AWS infrastructure management, enabling natural-language-driven operations. Whether you‚Äôre optimizing operations, experimenting with AI, or onboarding new team members ‚Äî via Amazon Q Developer CLI or any MCP host (Claude Desktop, Cursor, etc.) ‚Äî the CCAPI MCP Server and its companion tools provide an intuitive interface to AWS.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Huynh Trung Kien\nPhone Number: 0964325416\nEmail: kienhtse184454@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to \u0026hellip;\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://Kienht7704.github.io/Internship-Report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Workshop Overview In this workshop you will build a complete personal portfolio website using a serverless architecture on AWS ‚Äî no server management or scaling worries required.\nWorkshop Architecture The workshop leverages two main AWS services:\nAmazon S3 (Simple Storage Service) ‚Äî serves as the static web server for files such as HTML, CSS and images. S3 Static Website Hosting allows low-cost hosting and virtually unlimited storage.\nAmazon CloudFront ‚Äî a Content Delivery Network (CDN) that accelerates your site by delivering content from the edge location nearest to the user, instead of fetching it from a centralized origin. CloudFront also provides free HTTPS via AWS Certificate Manager.\nBenefits of this Architecture Performance: Content is served from the edge location closest to the user, reducing latency and improving page load times.\nCost: With the AWS Free Tier you get 12 months that include 5 GB of S3 storage, 1 TB of CloudFront transfer, and 10 million requests. After the Free Tier, hosting costs are typically very low (roughly ~$0.60/month for light traffic).\nOperations: No server management required ‚Äî no patching or manual scaling. AWS handles availability and scaling for you.\nSecurity: HTTPS is available by default, S3 bucket policies control access, and CloudFront includes AWS Shield Standard for basic DDoS protection.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/5-workshop/5.2-workshop-module1/","title":"Module 1","tags":[],"description":"","content":"Guide: Create Website Files for the Portfolio Objective In this module you will create two basic HTML files for your portfolio website:\nindex.html - the home page showing personal information error.html - the 404 error page when users access a non-existing path Step 1: Create a project folder Open File Explorer (Windows) or Finder (Mac). Create a new folder named my-portfolio. Open that folder with your text editor (VS Code, Notepad++, Sublime Text, etc.). Step 2: Create the index.html file 2.1. Create a new file Inside the my-portfolio folder, create a new file named index.html. 2.2. Copy the code into the file Open index.html and paste the following code:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Portfolio - Your Name\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; * { margin: 0; padding: 0; box-sizing: border-box; } body { font-family: \u0026#39;Segoe UI\u0026#39;, Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #333; } .container { max-width: 1200px; margin: 0 auto; padding: 0 20px; } header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 100px 0; text-align: center; } header h1 { font-size: 3em; margin-bottom: 10px; } header p { font-size: 1.2em; opacity: 0.9; } .section { padding: 60px 0; } .section:nth-child(even) { background: #f8f9fa; } h2 { color: #667eea; margin-bottom: 30px; font-size: 2em; } .skills { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin-top: 30px; } .skill-card { background: white; padding: 30px; border-radius: 10px; box-shadow: 0 5px 15px rgba(0,0,0,0.1); transition: transform 0.3s; } .skill-card:hover { transform: translateY(-5px); } .skill-card h3 { color: #667eea; margin-bottom: 10px; } .projects { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 30px; margin-top: 30px; } .project-card { background: white; border-radius: 10px; overflow: hidden; box-shadow: 0 5px 15px rgba(0,0,0,0.1); } .project-image { height: 200px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); display: flex; align-items: center; justify-content: center; color: white; font-size: 3em; } .project-content { padding: 20px; } .project-content h3 { color: #667eea; margin-bottom: 10px; } footer { background: #333; color: white; text-align: center; padding: 30px 0; } .contact-btn { display: inline-block; background: white; color: #667eea; padding: 12px 30px; border-radius: 25px; text-decoration: none; margin-top: 20px; font-weight: bold; transition: transform 0.3s; } .contact-btn:hover { transform: scale(1.05); } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;header\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Hello, I\u0026#39;m [Your Name]\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;AWS Solutions Architect | Cloud Engineer\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;section class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;About Me\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;I am a cloud engineer passionate about AWS, specializing in designing and deploying scalable cloud solutions. With experience working with EC2, S3, Lambda, and other AWS services, I am always looking to learn and apply new technologies.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;section class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Skills\u0026lt;/h2\u0026gt; \u0026lt;div class=\u0026#34;skills\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;skill-card\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;AWS Services\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;EC2, S3, Lambda, RDS, VPC, CloudFront, Route 53, IAM\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;skill-card\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Infrastructure as Code\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Terraform, CloudFormation, AWS CDK\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;skill-card\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Containers \u0026amp; Orchestration\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Docker, Kubernetes, ECS, EKS\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;skill-card\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Programming\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Python, JavaScript, Bash, SQL\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;section class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Featured Projects\u0026lt;/h2\u0026gt; \u0026lt;div class=\u0026#34;projects\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;project-card\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;project-image\u0026#34;\u0026gt;üåê\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;project-content\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Website Hosting on S3\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Deploy a static website with S3, CloudFront, and a custom domain. Low cost (\u0026amp;lt; $1/month) and fast load times (\u0026amp;lt; 1s).\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;project-card\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;project-image\u0026#34;\u0026gt;‚ö°\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;project-content\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Serverless API\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Build a RESTful API using Lambda, API Gateway, and DynamoDB. Auto-scaling with no server management.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;project-card\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;project-image\u0026#34;\u0026gt;üîê\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;project-content\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Security Automation\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Automate security audits using Lambda, CloudWatch, and SNS. Early detection and alerts for security issues.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;footer\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Contact\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Email: your.email@example.com\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;GitHub: github.com/yourusername\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;LinkedIn: linkedin.com/in/yourusername\u0026lt;/p\u0026gt; \u0026lt;a href=\u0026#34;mailto:your.email@example.com\u0026#34; class=\u0026#34;contact-btn\u0026#34;\u0026gt;Send Email\u0026lt;/a\u0026gt; \u0026lt;p style=\u0026#34;margin-top: 30px; opacity: 0.7;\u0026#34;\u0026gt;¬© 2024 - Hosted on AWS S3 + CloudFront\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 2.3. Customize personal information Replace the following with your own details:\nLine with [Your Name] ‚Üí replace with your name Job title line About section paragraph Contact info (email, GitHub, LinkedIn) 2.4. Save the file Use Ctrl + S (Windows) or Cmd + S (Mac) to save.\nStep 3: Create the error.html file 3.1. Create a new file In the same my-portfolio folder, create a file named error.html. 3.2. Copy the code into the file \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;404 - Page Not Found\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { font-family: Arial, sans-serif; display: flex; justify-content: center; align-items: center; height: 100vh; margin: 0; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; text-align: center; } h1 { font-size: 5em; margin: 0; } p { font-size: 1.5em; } a { color: white; text-decoration: underline; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h1\u0026gt;404\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Oops! The page you\u0026#39;re looking for cannot be found.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;/\u0026#34;\u0026gt;Return to the home page\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 3.3. Save the file Use Ctrl + S (Windows) or Cmd + S (Mac).\nStep 4: Quick local check Open File Explorer/Finder and locate index.html and error.html inside my-portfolio. Double-click a file to open it in your default browser and verify layout and content. Step 5: Final folder structure my-portfolio/ ‚îú‚îÄ‚îÄ index.html ‚îî‚îÄ‚îÄ error.html What you\u0026rsquo;ve completed Created two required HTML files Built a simple responsive layout Customized personal information placeholder Verified local testing steps Important notes Encoding: Save files as UTF-8 to avoid character issues. Filenames: Use lowercase index.html (S3 looks for this name as the default root file). If you want, I can also create a committed patch for these files or add translated versions of other workshop pages.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/5-workshop/5.3-workshop-module2/","title":"Module 2","tags":[],"description":"","content":"Upload Website to Amazon S3 Objective In this module you will:\nCreate an S3 bucket to store the website Upload the HTML files to S3 Configure S3 to host a static website Your site will get a public URL and be accessible from anywhere! Background What is S3? Amazon S3 (Simple Storage Service) is AWS\u0026rsquo;s object storage service. You can think of S3 as a huge \u0026ldquo;cloud hard drive\u0026rdquo; for storing files.\nWhat is S3 Static Website Hosting? S3 offers a feature that lets you host static websites (HTML, CSS, JS, images) without a web server. Key benefits:\nCheap (~$0.023/GB/month) No server management Automatic scaling for traffic spikes 11 nines of durability (99.999999999%) Step 1: Access the AWS Console 1.1 Sign in to AWS Open your browser and go to: https://console.aws.amazon.com Sign in with either: Root user email + password, OR IAM user (if provided by your organization) Select the region Asia Pacific (Singapore) ap-southeast-1 in the top-right corner Why Singapore? It\u0026rsquo;s geographically close to Vietnam, so it provides lower latency and reasonable pricing.\n1.2 Find the S3 service Type \u0026ldquo;S3\u0026rdquo; into the service search box in the console header Step 2: Create an S3 Bucket 2.1 Start creating the bucket In the S3 Console, click the orange Create bucket button 2. The bucket creation form includes several options 2.2 General configuration Bucket name:\nportfolio-yourname-2025 Important notes about bucket names:\nThe name must be globally unique (no other AWS account can use the same name) Use only lowercase letters, numbers and hyphens (-) No Vietnamese characters, no spaces Length between 3 and 63 characters Valid examples:\nportfolio-anhnguyen-2024 my-awesome-website-123 test-bucket-hcm AWS Region:\nAsia Pacific (Singapore) ap-southeast-1 2.3 Object ownership ACLs disabled (recommended) Explanation: ACLs are an older access control mechanism. AWS recommends using Bucket Policies for easier management.\n2.4 Block Public Access configuration IMPORTANT - READ CAREFULLY:\nBy default AWS blocks all public access for security. Because this bucket will host a public website, you need to disable those blocks for this bucket:\nUncheck all 4 options under \u0026ldquo;Block Public Access\u0026rdquo;:\nBlock all public access Block public access to buckets and objects granted through new access control lists (ACLs) Block public access to buckets and objects granted through any access control lists (ACLs) Block public access to buckets and objects granted through new public bucket or access point policies A yellow security warning will appear after you uncheck them\nSecurity warning: Only do this for a bucket that hosts a public website. Do not disable public access on buckets that store sensitive data.\n2.5 Finish Click the Create bucket button at the bottom of the page.\nStep 3: Upload Files to S3 3.1 Open the newly created bucket In the buckets list, click the name of the bucket you created You\u0026rsquo;ll land on the Objects view (file listing) 3.2 Upload files Click the orange Upload button Click Add files Select the two files: index.html and error.html from your my-portfolio folder Click Open You should see both files listed:\nüìÑ index.html üìÑ error.html 3.3 Keep default upload settings Scroll to the bottom and click Upload Wait for the progress bar to reach 100% You should see \u0026ldquo;Upload succeeded\u0026rdquo; for both files Click Close to return to the bucket view 3.5 Confirm upload You should now see the two files in the bucket:\nName Type Size Last modified index.html text/html ~8 KB Just now error.html text/html ~1 KB Just now Checkpoint: Files are uploaded to S3!\nStep 4: Enable Static Website Hosting 4.1 Open Properties tab While inside the bucket, click the Properties tab Scroll to the bottom 4.2 Find Static website hosting Locate the Static website hosting section ‚Äî it should display Disabled initially\n4.3 Enable and configure Click Edit inside the Static website hosting section Configure as shown in the screenshots Click Save changes 4.4 Save the Website Endpoint URL After saving, the Bucket website endpoint will appear. Example:\nhttp://portfolio-yourname-2025.s3-website-ap-southeast-1.amazonaws.com üìù Important: Copy this URL and save it (Notepad/Notes). The URL format is: http://[bucket-name].s3-website-[region].amazonaws.com\n‚úÖ Checkpoint: Static website hosting is enabled.\nStep 5: Configure Bucket Policy (Allow Public Read) 5.1 Why the site may still be inaccessible If you open the endpoint URL now you may get:\n403 Forbidden Access Denied This happens because objects are not yet publicly readable.\n5.2 Open the Permissions tab Click the Permissions tab Scroll to Bucket policy 5.3 Add a Bucket Policy Click Edit in the Bucket policy section Paste the following JSON into the editor: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::YOUR-BUCKET-NAME/*\u0026#34; } ] } Replace YOUR-BUCKET-NAME with your actual bucket name (e.g. portfolio-anhnguyen-2025) 5.4 What the policy does Explanation:\n\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; ‚Üí Allows access \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; ‚Üí Everyone \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34; ‚Üí Read/download objects \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::portfolio-anhnguyen-2025/*\u0026#34; Security note: This policy only grants read access; it does not allow delete, write, or upload operations.\n5.5 Save the policy Click Save changes\nCheckpoint: Bucket policy configured for public read access.\nStep 6: Test the Website 6.1 Open the website Open a new browser tab Paste the endpoint URL saved in step 4.4: http://portfolio-yourname-2025.s3-website-ap-southeast-1.amazonaws.com Press Enter 6.2 Expected result You should see the portfolio website with:\nA purple-blue gradient header Your name displayed Sections: About, Skills, Projects, Contact A footer at the bottom 6.3 Test the 404 page Open a non-existing path, e.g.:\nhttp://portfolio-yourname-2025.s3-website-ap-southeast-1.amazonaws.com/test123 You should see the 404 page with:\nA large \u0026ldquo;404\u0026rdquo; number Message \u0026ldquo;Oops! The page you\u0026rsquo;re looking for cannot be found.\u0026rdquo; A link back to the homepage 6.4 Share with others Your site is now live and accessible worldwide ‚Äî copy the URL and share it with friends for testing.\nüéâ Congratulations ‚Äî Module 2 Complete! What you accomplished: Created a globally unique S3 bucket Uploaded two HTML files to S3 Enabled Static Website Hosting Configured a public Bucket Policy Verified the site is online and tested the 404 page If you want, I can also create an English version of other workshop pages or make small edits to improve wording for documentation.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/5-workshop/5.4-workshop-module3/","title":"Module 3","tags":[],"description":"","content":"Add CloudFront - HTTPS \u0026amp; Global CDN Objective In this module you will:\nCreate a CloudFront distribution to deliver your website globally Enable free HTTPS for your website Speed up page load times by 5-10x Make your website more professional with a green lock icon in the address bar Background What is CloudFront? Amazon CloudFront is AWS\u0026rsquo;s Content Delivery Network (CDN) service.\nHow it works:\nUser in Hanoi ‚Üí CloudFront Edge (Hanoi) ‚Üí Return content instantly User in Tokyo ‚Üí CloudFront Edge (Tokyo) ‚Üí Return content instantly User in London ‚Üí CloudFront Edge (London) ‚Üí Return content instantly Instead of all users accessing S3 in Singapore, they access the CloudFront edge location nearest to them.\nWhy do you need CloudFront? 1. Free HTTPS (Security) S3 static websites only support HTTP CloudFront provides free HTTPS Green lock in browser ‚Üí Increased trust Google prioritizes HTTPS websites in search results 2. Faster load times Without CloudFront:\nUsers in Vietnam accessing S3 Singapore: ~50-100ms Users in Europe accessing S3 Singapore: ~300-500ms Users in the US accessing S3 Singapore: ~200-400ms With CloudFront:\nAll users: ~20-50ms (accessing the nearest edge location) 5-10x improvement! 3. More professional Shorter URL: d123abc.cloudfront.net vs bucket.s3-website-region.amazonaws.com Support for custom domains Advanced features (geo-blocking, custom headers, Lambda@Edge) Step 1: Access CloudFront Console 1.1 Open CloudFront From the AWS Console:\nType \u0026ldquo;CloudFront\u0026rdquo; in the service search box: 1.2 Welcome screen If this is your first time using CloudFront, you\u0026rsquo;ll see a welcome page.\nClick the orange Create distribution button.\nStep 2: Configure Origin (Content Source) Step 7: Test CloudFront 7.1 Copy Distribution Domain Name In the distributions list or in the distribution details:\nDomain name: d1234567890abcd.cloudfront.net Copy this domain.\n7.2 Access the website via HTTPS Open your browser and enter:\nhttps://d1234567890abcd.cloudfront.net Note: Remember to add https:// at the beginning!\n7.3 Expected result Website displays correctly:\nPurple-blue gradient header All content renders properly Layout is not broken Green lock in the address bar:\nhttps://d1234567890abcd.cloudfront.net Click the lock to see:\nConnection is secure Certificate is valid 7.4 Test HTTP to HTTPS redirect Try accessing with HTTP (no \u0026rsquo;s\u0026rsquo;):\nhttp://d1234567890abcd.cloudfront.net The browser will automatically redirect to:\nhttps://d1234567890abcd.cloudfront.net 7.5 Test 404 page Access a non-existent URL:\nhttps://d1234567890abcd.cloudfront.net/test-404 Your error.html page will display (large \u0026ldquo;404\u0026rdquo; with gradient background)\nUnderstanding CloudFront Better How caching works First request:\nUser ‚Üí CloudFront Edge ‚Üí (MISS) ‚Üí S3 ‚Üí CloudFront ‚Üí User ‚îî‚îÄ Save to cache Subsequent requests:\nUser ‚Üí CloudFront Edge ‚Üí (HIT) ‚Üí User (Serve from cache instantly) Cache duration Default: 24 hours (using CachingOptimized policy)\nAfter 24 hours, CloudFront checks S3 to see if files have changed.\nWhat is Invalidation? When you update files on S3, CloudFront still serves the old cached version.\nHow to force CloudFront to update immediately:\nGo to CloudFront Console ‚Üí Select your distribution Go to \u0026ldquo;Invalidations\u0026rdquo; tab ‚Üí Click \u0026ldquo;Create invalidation\u0026rdquo; Object paths: /* (invalidate all) Click \u0026ldquo;Create invalidation\u0026rdquo; Wait 1-2 minutes ‚Üí Fresh cache! Note: The Free Tier includes 1,000 free invalidation paths per month.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material MON - Form a group and connect with group members - Spend time reading and memorizing the regulations before going to the internship office - Watch introduction videos about AWS on YouTube: + What is AWS and Cloud Computing? + AWS Global Infrastructure + Cost Optimization on AWS + \u0026hellip; 08/09/2025 08/09/2025 https://www.youtube.com/@AWSStudyGroup TUE - Create an AWS Free Tier account \u0026amp; set up MFA following the provided documentation - Learn how to use AWS Console: + Create Admin group and Admin user + Set up security for the Admin user + Configure region and initialize dashboard widgets + Learn and clearly understand AWS support plans 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/ https://www.youtube.com/@AWSStudyGroup WED - Learn about EC2: + What is EC2? + AMI + EBS Volume + Snapshot + Instance + Security Group (Access restriction) 10/09/2025 10/09/2025 https://www.youtube.com/watch?v=6PqZVGoeEEA\u0026list=PLgT9f4ZU9cncuJ5ACqQxook4yKO1xuZP6\u0026index=3 THU - Review theory on Subnet, Route Table, Internet Gateway, NAT Gateway - Practice: + Create VPC, Subnet, Internet Gateway, Route Table, and Security Group + Launch EC2 with instance type: t3.micro + Connect to the EC2 Private instance + Create a NAT Gateway 11/09/2025 11/09/2025 https://000003.awsstudygroup.com/vi/4-createec2server/ https://www.youtube.com/watch?v=6PqZVGoeEEA\u0026list=PLgT9f4ZU9cncuJ5ACqQxook4yKO1xuZP6\u0026index=4 FRI - Learn about Site-to-Site VPN - Practice: + Create VPC for VPN + Create VPG and Customer Gateway + Establish a VPN connection + Configure and Customize Customer Gateway \u0026amp; AWS VPN Tunnel 12/09/2025 12/09/2025 https://000003.awsstudygroup.com/vi/5-vpnsitetosite/ Week 1 Achievements: General Tasks Formed a team and connected with members. Understood and memorized internship regulations. AWS Fundamentals Gained knowledge via introduction videos: Cloud \u0026amp; AWS concepts AWS Global Infrastructure Cost Optimization on AWS AWS Account Management Successfully created an AWS Free Tier account and enabled MFA. Used AWS Console to: Create Admin group and Admin user Configure Admin user security Set region and dashboard widgets Learn about AWS Support Plans EC2 Acquired knowledge of EC2, AMI, EBS, Snapshot, Instance, Security Group. Basic Networking Infrastructure Practiced: Creating VPC, Subnet, Internet Gateway, Route Table, Security Group Launching EC2 (t3.micro) Connecting to Private EC2 instance Creating a NAT Gateway VPN Learned and practiced configuring Site-to-Site VPN: Created VPC for VPN Created VPG and Customer Gateway Established VPN Tunnel Customized connection between Customer Gateway \u0026amp; AWS VPN Tunnel "},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Budget: + Cost Budget + Usage Budget + Reservation Instance (RI) Budget + Savings Plans Budget - Learn about IAM (Identity and Access Management) + Authentication \u0026amp; Authorization + IAM User + IAM Groups + IAM Policies + IAM Roles Hands-on Practice: - Initialize Budget - Create IAM Groups, User, Role \u0026amp; Sign in with IAM User - Switch between IAM Roles 15/09/2025 15/09/2025 https://000007.awsstudygroup.com/vi/ https://000002.awsstudygroup.com/vi/ 3 - Review and study the theory of EC2, subnet, VPC, Internet Gateway, Nat Gateway, route table, Security Group, Region 16/09/2025 16/09/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Learn about S3: S3 Bucket: + Is a container holding objects + Named uniquely across the globe S3 Object: + Is a file/data stored inside a Bucket + Size ranges from 0 bytes to 5TB per object + Unlimited objects per Bucket S3 Versioning: + Protects data (keeps versions of objects in the bucket) + Recover data from previous versions S3 Cross-Region Replication (CRR) + Replicate Buckets across different AWS regions Hands-on Practice: Create S3 Bucket and upload source data + Run website \u0026amp; integrate with CloudFront + Move Objects across Buckets + Replicate S3 Objects to another region 17/09/2025 17/09/2025 https://000057.awsstudygroup.com/vi/1-introduce/ 5 - Learn about Amazon Relational Database Service (RDS) + Deploy and manage relational databases on AWS - Amazon RDS supports databases: + Amazon Aurora + MySQL + MariaDB + Oracle + SQL Server + PostgreSQL Hands-on Practice: + Create Security Group for DB instance, Create DB Subnet group Install Git \u0026amp; Node.js on EC2 + Create DB instance + Deploy application 18/09/2025 18/09/2025 https://000005.awsstudygroup.com/ 6 - Learn about EC2 Auto Scaling 19/09/2025 19/09/2025 https://000006.awsstudygroup.com/vi/ Week 2 Achievements: Cost Management (AWS Budgets) Learned about budget types: Cost, Usage, Reservation Instance (RI), and Savings Plans. Practiced initializing Budgets to monitor costs. Identity and Access Management (IAM) Understood the concepts of Authentication \u0026amp; Authorization. Mastered the components: IAM User, Groups, Policies, Roles. Hands-on practice: Created IAM Groups, Users, Roles. Signed in with IAM User and switched between IAM Roles. Network Infrastructure Review Reviewed and consolidated knowledge of: EC2, VPC, Subnet, Internet Gateway, NAT Gateway, Route Table, Security Group. Storage Service (S3) Acquired knowledge about S3 Bucket and Object. Understood Versioning (version management) and Cross-Region Replication (CRR) features. Hands-on practice: Created Bucket, uploaded data. Deployed Static Website integrated with CloudFront. Moved Objects and replicated data to another Region. Database (RDS) Learned about Amazon RDS and supported databases (Aurora, MySQL, PostgreSQL, etc.). Hands-on practice: Configured Security Group and DB Subnet group. Installed Git \u0026amp; Node.js on EC2. Initialized DB Instance and deployed an application connected to the database. Scalability Learned about EC2 Auto Scaling mechanism. View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel. \u0026hellip; "},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS CloudWatch service 22/09/2025 22/09/2025 https://000008.awsstudygroup.com/vi/ 3 - Learn about Route 53 23/09/2025 23/09/2025 https://000010.awsstudygroup.com/vi/ 4 - Get familiar with AWS CLI 24/09/2025 24/09/2025 https://000011.awsstudygroup.com/vi/ 5 - Watch Module 3 on YouTube AWS Study Group 25/08/2025 25/08/2025 https://www.youtube.com/@AWSStudyGroup 6 - Review and consolidate theory of AWS CloudWatch, Route 53, and Module 3 concepts 26/08/2025 26/08/2025 \u0026lt;\u0026gt; Week 3 Achievements: Monitoring and Management Learned about AWS CloudWatch service: Monitor resources and applications on AWS. Collect and track metrics. Domain Name System (DNS) Researched Amazon Route 53: Understand traffic routing mechanisms. Manage domain names and DNS records. Management Tools Became familiar with AWS CLI (Command Line Interface): Install and configure the environment. Interact with AWS services via command line instead of Console. Review and Knowledge Consolidation Completed Module 3 learning content (according to AWS Study Group curriculum).\nSystematized all theory learned during the week:\nAWS CloudWatch Route 53 Key concepts from Module 3. Check account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel. \u0026hellip; "},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Hands-on: - Build a simple static website using HTML and CSS - Host it on S3 and distribute via Amazon CloudFront and add a custom domain with Route 53 29/09/2025 29/09/2025 3 - Review theory from Module 1 to Module 3 30/09/2025 30/09/2025 4 - Watch Module 4 theory on the AWS Study Group YouTube channel 01/10/2025 01/10/2025 https://www.youtube.com/watch?v=hsCfP0IxoaM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=103 5 - Follow and practice the AWS Study Group videos: - Deploy Infrastructure - Create Backup Plan - Set up notifications 02/10/2025 02/10/2025 https://www.youtube.com/watch?v=ZQtDG-DBiYw\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=108 https://www.youtube.com/watch?v=cmIMSqeqPr4\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=109 https://www.youtube.com/watch?v=I9ISH11xIS8\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=110 6 - Follow and practice the AWS Study Group videos: - Test Restore - VMWare Workstation 03/10/2025 03/10/2025 https://www.youtube.com/watch?v=fZeMSyE0Spc\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=111 https://www.youtube.com/watch?v=lRbXC9UXqdo\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=112 https://www.youtube.com/watch?v=Yr6oD4btfZg\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=113 Week 4 Achievements: Static Website Deployment Practiced building and distributing a complete static website: Created a simple site using HTML and CSS. Uploaded source files to an S3 Bucket for hosting. Used Amazon CloudFront as a CDN to accelerate content delivery. Configured a custom domain using Route 53. Review and Theory Consolidated knowledge from Module 1 through Module 3. Studied new theory from Module 4 (per AWS Study Group path). Infrastructure Management \u0026amp; Backup (Backup \u0026amp; Restore) Practiced operational procedures and data protection: Deploy Infrastructure: deploy network/compute resources. Create Backup Plan: set up automated backup strategies. Set up Notifications: configure status/alerting systems. Test Restore: perform restore tests to ensure data integrity. Virtualization Tools Explored and practiced using VMWare Workstation. "},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review what was learned in previous weeks 6/10/2025 6/10/2025 3 - Learn about Amazon Lightsail 7/10/2025 7/10/2025 https://000045.awsstudygroup.com/vi/ 4 - Follow and practice AWS Study Group videos: - Test Restore - VMWare Workstation 8/10/2025 8/10/2025 https://www.youtube.com/watch?v=fZeMSyE0Spc\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=111 https://www.youtube.com/watch?v=lRbXC9UXqdo\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=112 https://www.youtube.com/watch?v=Yr6oD4btfZg\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=113 5 - Follow and practice AWS Study Group videos 9/10/2025 9/10/2025 https://www.youtube.com/watch?v=ZnYmQv39UXY\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=142 https://www.youtube.com/watch?v=AoUn_23FYnY\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=143 https://www.youtube.com/watch?v=n3uovWBqA88\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=144 6 - Follow and practice AWS Study Group videos 10/10/2025 10/10/2025 https://www.youtube.com/watch?v=AfhH0s_CUVg\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=145 https://www.youtube.com/watch?v=85Q1Tgdey18\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=146 https://www.youtube.com/watch?v=DRuSUMEezSk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=147 Week 5 Achievements: Virtual Private Server (VPS) Learned about and practiced with Amazon Lightsail: Deploy web applications quickly. Manage virtual servers with optimized costs. Backup \u0026amp; Virtualization Practiced data recovery procedures (Test Restore) to ensure availability. Used VMware Workstation to manage local virtualization environments. Web Optimization \u0026amp; Security (Lab 57) Continued hands-on practice (Module 4 - Lab 57): Test Website: Ensure application runs smoothly after deployment. Security: Configure Block all public access to prevent unauthorized access to storage resources. Content Distribution: Test integration with Amazon CloudFront to optimize access speeds. Version Management: Enable S3 Bucket Versioning to protect and recover data versions. Review Systematized and consolidated all knowledge learned over the first 4 weeks. "},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch Module 5 theory on YouTube AWS Study Group 13/10/2025 13/10/2025 https://www.youtube.com/watch?v=tsobAlSg19g\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=150 3 - Follow and practice AWS Study Group videos 14/10/2025 14/10/2025 https://www.youtube.com/watch?v=YnLo4MgOXyA\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=158 https://www.youtube.com/watch?v=uIv0GC1XWMY\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=159 https://www.youtube.com/watch?v=jPmjicPl1js\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=160 4 - Participate in event: AWS Cloud Mastery Series #1 15/10/2025 15/10/2025 5 - Follow and practice AWS Study Group videos 16/10/2025 16/10/2025 https://www.youtube.com/watch?v=esPRIj_zZSQ\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=161 https://www.youtube.com/watch?v=k-g4wz4Qlfk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=162 https://www.youtube.com/watch?v=BivRALFsoxQ\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=163 6 - Participate in event: AWS Cloud Mastery Series #2 17/10/2025 17/10/2025 Week 6 Achievements: Security \u0026amp; Compliance Knowledge Mastered the Shared Responsibility Model between AWS and customers. Learned about key security and governance services: AWS Identity and Access Management (IAM): Manage identities and access permissions. Amazon Cognito: Manage authentication for mobile and web applications. AWS Organizations \u0026amp; IAM Identity Center: Manage multi-account environments and single sign-on (SSO). AWS Key Management Service (KMS): Manage data encryption keys. Lab Practice (Module 5) Security Posture Management: Practiced enabling and configuring AWS Security Hub for automated compliance and security checks. Secure Network Infrastructure: Practiced setting up VPC (Virtual Private Cloud) as a foundation for advanced security labs. Event Participation Participated in AWS Cloud Mastery Series events: Series #1: (15/10/2025) Series #2: (17/10/2025) "},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review knowledge learned 20/10/2025 20/10/2025 3 - Follow and practice AWS Study Group videos 21/10/2025 21/10/2025 https://www.youtube.com/watch?v=j9hGJXIDdBQ\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=164 https://www.youtube.com/watch?v=AVhWQOYnj14\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=165 https://www.youtube.com/watch?v=7_1A2Jxl7_c\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=166 https://www.youtube.com/watch?v=-iGkIHh9mXo\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=167 https://www.youtube.com/watch?v=aq-YDewEyhA\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=168 https://www.youtube.com/watch?v=lwTy-DeCuBY\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=169 https://www.youtube.com/watch?v=HdcBW-eKJWk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=170 4 - Follow and practice AWS Study Group videos 22/10/2025 22/10/2025 https://www.youtube.com/watch?v=ZQsPVBH8m78\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=171 https://www.youtube.com/watch?v=8pZ_PJAFL74\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=172 https://www.youtube.com/watch?v=_0u1kdborw4\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=173 https://www.youtube.com/watch?v=kYyceAFqsUg\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=174 https://www.youtube.com/watch?v=YDAXEiY8TzU\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=175 https://www.youtube.com/watch?v=igwP7gx91Cs\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=176 5 - Follow and practice AWS Study Group videos 23/10/2025 23/10/2025 https://www.youtube.com/watch?v=vkBJ0Cxc6Nw\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=177 https://www.youtube.com/watch?v=XXjEpIRWJBk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=178 https://www.youtube.com/watch?v=pJWANviu8QM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=179 https://www.youtube.com/watch?v=pJWANviu8QM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=179 https://www.youtube.com/watch?v=pJWANviu8QM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=179 https://www.youtube.com/watch?v=gbJdWHt-ScM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=185 6 - Exam review 24/10/2025 24/10/2025 Week 7 Achievements: Automation \u0026amp; Monitoring Completed hands-on Lab 22: Integrated Incoming Webhooks Slack for notifications. Managed Tags for Instances. Created IAM Role for AWS Lambda. Wrote Function to automatically Stop/Start Instance to optimize costs. Verified results and cleaned up resources. Efficient Resource Management Performed hands-on Lab 27 on Tagging Strategy: Created EC2 Instance with Tags. Managed Tags for AWS resources. Used Filter to search resources by tag. Used AWS CLI to manipulate Tags. Created Resource Group to group related resources. Identity \u0026amp; Access Management (IAM Deep Dive) Continued with hands-on Lab 28: Created IAM User and custom IAM Policy. Initialized IAM Role and verified security policies (Policy Check) to ensure correct permission delegation. Review \u0026amp; Exam Preparation Spent early and late week reviewing all learned knowledge. Prepared for competency assessment exam. "},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Exam review 27/10/2025 27/10/2025 3 - Exam review 28/10/2025 28/10/2025 4 - Participate in event: AWS Cloud Mastery Series #3 29/10/2025 29/10/2025 \u0026lt;\u0026gt; 5 - Exam review 30/10/2025 30/10/2025 6 - Take exam 31/10/2025 31/10/2025 \u0026lt;\u0026gt; Week 8 Achievements: Exam Preparation Reviewed AWS fundamentals and concepts in preparation for the exam. "},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory on YouTube AWS Study Group 03/11/2025 03/11/2025 https://www.youtube.com/watch?v=OOD2RwWuLRw\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=217 3 - Analyze Project - Identify workload for the project + AWS architecture for the project 04/11/2025 04/11/2025 4 - Write Proposal for the project 05/11/2025 05/11/2025 5 - Start building UI for the project on Figma 06/11/2025 06/11/2025 6 - Build UI for Project 07/11/2025 07/11/2025 Week 9 Achievements: Database Fundamentals Knowledge Watched and mastered theory through Module 06-01 - Database Concepts review video: Core concepts: RDBMS vs NoSQL, OLTP vs OLAP. Understood Data Warehouse, Indexing, Partitioning, and query optimization strategies. Project Analysis \u0026amp; System Design Project Analysis: Evaluated requirements and identified workload for the project. AWS Architecture: Designed cloud infrastructure diagram suitable for the project. Documentation \u0026amp; Design Proposal Writing: Completed project proposal document to present ideas and solutions. UI Design: Used Figma to build user interface (UI) for the application. "},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/4-eventparticipated/4.2-event2/","title":"Discover Agentic AI ‚Äì Amazon QuickSuite Workshop","tags":[],"description":"","content":"Discover Agentic AI ‚Äì Amazon QuickSuite Workshop - Date: November 7, 2025 - Location: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nEvent Overview An exclusive workshop focused on the shift from passive Generative AI to autonomous Agentic AI. The event featured the first live demonstration of Amazon QuickSuite in Vietnam and introduced the AWS LIFT Program to lower financial barriers for adoption.\nKey Objectives:\nDefine Agentic AI: Clarify the concept of autonomous AI agents that can reason and execute tasks. Introduce Amazon QuickSuite: Showcase the unified data visualization (QuickSight) and generative AI (Quick Suite Q) platform. Enable Hands-on Learning: Provide a practical environment to build AI concepts with expert guidance. Facilitate Adoption: Offer an $80,000 USD credit through the AWS LIFT Program to accelerate R\u0026amp;D. Key Takeaways \u0026amp; Learnings Focus on Autonomy: The design goal of Agentic AI is to build systems that act on a user\u0026rsquo;s behalf, not just provide information. Ecosystem Approach is Crucial: Effective agents require a connected network of tools, like the one provided by QuickSuite, to link data sources with action logic. Early Adoption Creates Advantage: Gaining proficiency with tools like QuickSuite before they become mainstream offers a significant competitive edge. Funding Accelerates Innovation: Financial incentives like the LIFT program enable companies to experiment and innovate more quickly. Application to Work Explore QuickSuite for Analytics: Investigate integrating QuickSight and Quick Suite Q to create \u0026ldquo;Analyst Agents\u0026rdquo; that can automate data reporting and analysis. Secure R\u0026amp;D Funding: Apply for the AWS LIFT Program to secure credits for upcoming AI-related research and development projects. Identify Automation Use Cases: Audit internal operations to find repetitive, multi-step tasks suitable for autonomous execution by an AI agent. Engage with Implementation Partners: Collaborate with partners like Cloud Kinetics for complex architectural design and implementation, reducing in-house development risks. Event Photos Add your event photos here\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Aurora Time Unified AWS Serverless Solution for Personal Time Management 1. Executive Summary This proposal presents the implementation plan for Aurora Time, a time management application on the AWS platform, simple and focused on scheduling features, to address the complexity and operational costs of current solutions. Aurora Time will leverage AWS serverless and managed services to ensure high scalability, reliability, and cost optimization, delivering rapid return on investment (ROI) through reduced infrastructure management costs.\n2. Problem Statement Current Problem Individuals struggle to manage daily commitments because schedules are scattered (notes, phones) leading to confusion and missed tasks. Current tools are often too complex, overloaded with work features, and unsuitable for the need for quick and simple scheduling in personal life. Aurora Time solves this by providing a centralized, minimalist, and intuitive platform to easily track habits and important milestones.\nSolution The platform uses Amazon S3 combined with Amazon CloudFront to store and distribute web applications, using AWS Amplify for rapid development and deployment. Amazon API Gateway and AWS Lambda serve as the Backend processing layer to handle event CRUD requests. Data is stored in Amazon DynamoDB to ensure fast access speed and low latency. Amazon Cognito ensures secure authentication and authorization for each individual user. Amazon EventBridge and Amazon SES are used to trigger and send scheduled reminder notifications. Similar to other calendar applications, users can create and edit personal schedules, but this platform operates at a more minimalist scale and serves the purpose of daily personal time management. Key features include intuitive scheduling interface, customizable reminders, and low operational costs.\nBenefits and Return on Investment (ROI) The Aurora Time solution creates a solid foundation for individual users to centralize all schedules while providing a cost-saving Serverless architecture model for easy feature expansion. The platform reduces schedule fragmentation and minimizes missed important commitments through a centralized, simple reminder system, simplifying personal time management and improving work/life balance.\nEstimated monthly infrastructure costs of $16 - $50 USD, totaling approximately $192 - $600 USD for 12 months (depending on number of users and requests). All development components are based on Serverless, incurring no hardware purchases or 24/7 virtual server rental costs. The payback period is estimated at under 6 months thanks to significant time savings in searching and manually arranging schedules, and the extremely low operational costs of the AWS Serverless architecture.\n3. Solution Architecture The platform applies AWS Serverless architecture to manage personal schedule and event data, with the capability to easily scale from a single user to millions of individual users. API requests are received through Amazon API Gateway and processed by AWS Lambda, while data is stored in Amazon DynamoDB to ensure fast query speed and low latency. Amazon EventBridge handles reminder scheduling logic, triggers Lambda, and sends notifications. AWS Amplify provides an intuitive web/mobile interface, secured by Amazon Cognito to safely manage access permissions for each user.\nAWS Services Used AWS Lambda: Processes business logic for event CRUD operations and triggers scheduled reminder tasks. Amazon API Gateway: Provides secure RESTful API interface for communication with web applications. Amazon DynamoDB: Stores event data, schedules, and user information. Amazon S3 and CloudFront: Stores and distributes static content of Frontend applications. Amazon EventBridge: Schedules and triggers automatic reminder events at user-defined times. Amazon SES: Sends customized reminder notifications via email (SES). AWS Amplify: Stores and provides intuitive web interface. Amazon Cognito: Manages access permissions and secure authentication for individual users. Component Design User Interface: AWS Amplify hosts web applications (planned to use React) providing intuitive scheduling interface, schedule viewing, and reminder settings. User Authentication: Amazon Cognito manages individual user accounts, issuing secure authorization tokens for Backend API access. API Request Reception: Amazon API Gateway receives and routes authenticated requests (e.g., create event, query schedule) to corresponding Lambda functions. Backend Logic Processing: AWS Lambda processes and executes business logic. Meanwhile, other Lambda functions are triggered by EventBridge to send reminders. Data Storage: Amazon DynamoDB stores primary data (Schedules, Events, User Information) in NoSQL format, ensuring fast access and flexibility. Reminder Scheduling: Amazon EventBridge schedules and triggers events at user-defined times, ensuring reminder features operate automatically. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases\nThe project consists of 2 parts ‚Äî Backend Serverless setup and Frontend User Interface building ‚Äî each part goes through 4 phases:\nResearch and Architecture Design: In-depth research on DynamoDB Data Modeling for schedules and design of verified AWS Serverless architecture (API Gateway, Lambda, EventBridge). (Timeline: Month 1) Cost Calculation and Feasibility Check: Use AWS Pricing Calculator to estimate actual Serverless operational costs and verify feasibility of authentication (Cognito) and storage (DynamoDB) flows. (Timeline: Month 1) Architecture Adjustment for Cost/Solution Optimization: Fine-tune Lambda parameters (e.g., memory, timeout) and DynamoDB (e.g., RCU/WCU) to ensure highest cost efficiency and best performance for individual users. (Timeline: Month 2) Development, Testing, Deployment: Program Lambda functions, set up CI/CD Pipeline with CodePipeline/CodeBuild/CloudFormation, and develop Frontend applications (React). Then conduct Beta testing and go live. (Timeline: Month 2-3) Technical Requirements\n1. Backend Requirements (Serverless)\nCore Services: In-depth knowledge of AWS Lambda (Node.js), Amazon DynamoDB, Amazon API Gateway, and Amazon Cognito. Event Management: Proficiency in Amazon EventBridge to schedule and trigger Lambda functions sending reminders via Amazon SES/SNS. 2. Frontend Requirements\nInterface: Practical knowledge of AWS Amplify to host React applications and connect to API Gateway Optimization: Leverage React\u0026rsquo;s processing capabilities to reduce load on Lambda functions 5. Roadmap \u0026amp; Deployment Milestones Internship (Month 1‚Äì3): Month 1: Learn AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Deploy, test, and go live. Post-Deployment: Further research over 1 year. 6. Budget Estimation Infrastructure Costs\nAWS Amplify: $0.35/month S3 Standard: $0.05/month CloudFront: $1.70/month API Gateway: $0.11/month AWS Lambda: $0.00 (free) /month DynamoDB: $0.11/month Amazon Cognito: $0.00/month Amazon SES (email): $0.05/month EventBridge: $0.10/month CloudWatch Logs: $0.10/month CI/CD Pipeline + Build: $0.00 (free) /month Total: $2.57/month, $30.84/12 months\n7. Risk Assessment Risk Matrix\nNetwork disconnection/High latency: Medium impact, medium probability. DynamoDB design error: High impact, medium probability. Serverless costs exceed budget: Medium impact, low probability. Reminder system failure: High impact, low probability. Mitigation Strategies\nNetwork disconnection/High latency: Optimize Frontend (stored on S3/CloudFront) to ensure fast loading speeds, use Client-side Caching so users can still view recent schedules when network is lost. DynamoDB design error: Conduct strict Proof of Concept (POC) and Load Testing for data models. Use optimized Global Secondary Index (GSI) to avoid querying entire table and minimize RCU/WCU costs. Serverless costs exceed budget: Set up AWS Budgets with proactive alerts (Email/SNS) when spending approaches threshold. Regularly check AWS Cost Explorer and optimize Lambda resources. Reminder system failure: Closely monitor EventBridge and Lambda reminder processing through CloudWatch Alarms to detect errors immediately. Implement Retry Logic for critical Lambda functions. Contingency Plan\nAWS Incident: Since the application is only for individuals, if AWS encounters an incident, the system will be configured to automatically Rollback to the latest code version (via CodePipeline) or re-deploy configuration (via CloudFormation). Data Loss: Set up DynamoDB Backup and Restore at regular intervals to quickly recover event data in case of system failure or user error. Cost Issues: Use CloudFormation to restore resource configuration (such as RCU/WCU) to proven low-cost state. 8. Expected Results User Experience Improvement: Provide intuitive scheduling interface and real-time notifications (Real-time notifications) replacing manual note-taking and schedule tracking processes. The system is designed for easy scaling, serving from a single user to millions of individual users.\nLong-term Value and Reusability: Create a solid Serverless technical platform that can maintain operation at extremely low costs for many years. This architecture can be reused for personal application projects or other expansion features in the future (e.g., habit tracking, simple personal task management).\nSuccessful Deployment: Successfully deploy the entire approved Serverless architecture, including automated CI/CD and core features of Aurora Time within the planned timeframe.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch video and practice Lab 43 10/11/2025 10/11/2025 https://www.youtube.com/watch?v=cxwAOP1379s\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=229 3 - Review what was learned 11/11/2025 11/11/2025 4 - Build FrontEnd for Project 12/11/2025 12/11/2025 5 - Build FrontEnd for Project 13/11/2025 13/11/2025 6 - Build FrontEnd for Project 14/11/2025 14/11/2025 Week 10 Achievements: Lab 43 Practice (EC2 \u0026amp; RDP) Watched tutorial video and practiced Lab 43: EC2 Connect RDP Client: Connected to EC2 virtual server (typically Windows Server) using Remote Desktop Protocol (RDP). Retrieved Administrator password from Key pair and established connection from workstation. Knowledge Review Spent time consolidating all learned knowledge and skills to build a strong foundation for the project development phase. Project Development (FrontEnd Development) Built user interface (FrontEnd): Focused weekend time on building the FrontEnd for the project. Ensured interface compatibility and correct functionality according to the finalized UI design. "},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Implement login using Amazon Cognito 17/11/2025 17/11/2025 3 - Rewrite project proposal 18/11/2025 18/11/2025 4 - Fix project code 19/11/2025 19/11/2025 \u0026lt;\u0026gt; 5 - Conduct project end-to-end testing (round 1) 20/11/2025 20/11/2025 \u0026lt;\u0026gt; 6 - Review codebase and verify main flows 21/11/2025 21/11/2025 \u0026lt;\u0026gt; Week 11 Achievements: Completed the login feature. Proposal Template Update Completed the new proposal template. Project Testing Identified issues in time. Fixed critical bugs. "},{"uri":"https://Kienht7704.github.io/Internship-Report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Prepare workshop content and finalize the proposal 24/11/2025 24/11/2025 3 - Test frontend ‚Üí Amplify API ‚Üí Lambda ‚Üí Resend + Cognito login successful + Send OTP/notification from frontend 25/11/2025 25/11/2025 4 - Create presentation slides - Review and optimize presentation content - Standardize images for the presentation 26/11/2025 26/11/2025 \u0026lt;\u0026gt; 5 - Test all possible project scenarios 27/11/2025 27/11/2025 6 - Final review of all documents and slides + Prepare demo flow and verify demo operation 28/11/2025 28/11/2025 Week 12 Achievements: Project review and bug checks Features operate smoothly and stably. No critical bugs were found. Proposal completion Structure refined and clarified. Descriptions completed. Formatting standardized. Presentation slides Content concise and well-illustrated. Ready for the final report presentation. "},{"uri":"https://Kienht7704.github.io/Internship-Report/4-eventparticipated/4.3-event3/","title":"AWS Cloud Mastery Series #3 - Security Pillar Deep Dive","tags":[],"description":"","content":"AWS Cloud Mastery Series #3 - Security Pillar Deep Dive - Date: December 1, 2025 - Location: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nEvent Overview An in-depth workshop focused on the Security Pillar of the AWS Well-Architected Framework. The event provided knowledge and best practices for securing cloud workloads.\nKey Objectives:\nDeep Dive into the Security Pillar: Analyze the design principles and key areas of security on AWS. Identity and Access Management: Gain a deep understanding of AWS IAM, MFA, and best practices for access control. Data Protection: Explore techniques for encrypting data at-rest and in-transit. Automation and Monitoring: Learn how to use AWS Config, CloudTrail, and Security Hub to monitor and automate security controls. Key Takeaways \u0026amp; Learnings Security is a Shared Responsibility: Understand the shared responsibility model and the customer\u0026rsquo;s role in securing applications in the cloud. Defense in Depth: Apply multiple layers of security to protect resources comprehensively. Automation is Key: Automating security checks and remediation reduces human error and allows for faster responses to threats. Application to Work Re-evaluate IAM Policies: Review and strengthen existing IAM policies according to the principle of least privilege. Implement Security Monitoring: Set up AWS Security Hub for a centralized, comprehensive view of the security posture. Enhance Data Encryption: Ensure all sensitive data is encrypted using AWS KMS. "},{"uri":"https://Kienht7704.github.io/Internship-Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - AWS Powers Breakthrough Gaming Experiences at devcom and gamescom 2025 This post summarizes AWS activities and sessions at devcom and gamescom 2025. It highlights AWS for Games programs, demos, and workshops‚Äîcovering GameLift and GameLift Streams, AI-powered QA workflows, localization and continuous localization tooling, game analytics pipelines, and partner integrations. The article is useful for game developers interested in cloud-powered streaming, scalability, and AI-driven development workflows.\nBlog 2 - Flexibility to Framework: Building MCP Servers with Controlled Tool Orchestration This article explains the Model Context Protocol (MCP) and addresses the challenge of enforcing tool execution order when agents call multiple tools. The author presents a token-based orchestration pattern: the server issues short-lived verification tokens, tools validate tokens against a server-side store, and tokens are consumed/rotated to enforce a secure sequential workflow (generate ‚Üí scan ‚Üí create). The post also describes @mcp.tool() patterns, dependency discovery, and trade-offs for session and concurrency handling.\nBlog 3 - Introducing AWS Cloud Control API MCP Server This post introduces the CCAPI MCP Server, which enables natural-language-driven infrastructure management by using the AWS Cloud Control API to perform CRUDL operations across many AWS resource types. Key features covered include IaC export, cost estimation via the Pricing API, automated security checks with Checkov, read-only mode, and a practical example (create an S3 bucket encrypted with KMS). The article highlights how the server integrates with developer tools (for example Amazon Q Developer) to streamline intent-to-infrastructure workflows.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"This section lists the events you have participated in during your internship. For example:\n4.1. AWS Cloud Day Vietnam - AI Edition 2025 Date: September 18, 2025\nLocation: 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nA pivotal gathering for Vietnam\u0026rsquo;s tech and business communities, focused on accelerating digital transformation through the convergence of Cloud Computing and Artificial Intelligence.\n4.2. Discover Agentic AI ‚Äì Amazon QuickSuite Workshop Date: November 7, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nAn exclusive workshop focused on the shift from passive Generative AI to autonomous Agentic AI. The event featured the first live demonstration of Amazon QuickSuite in Vietnam and introduced the AWS LIFT Program to lower financial barriers for adoption.\n4.3. AWS Cloud Mastery Series #3 - Security Pillar Deep Dive Add your event details here\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://Kienht7704.github.io/Internship-Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"Throughout my internship at AWS from 08/09/2025 to 12/12/2025, I had the opportunity to learn, train, and apply the knowledge acquired at university to a real-world work environment. I participated in developing and deploying a complete Serverless web application on AWS Cloud, including setting up the access flow (Route 53, CloudFront), security (API Gateway, Cognito), backend logic (Lambda, DynamoDB), and automating the deployment process (CloudFormation, CodePipeline)., thereby improving the following skills: Serverless Programming (AWS Lambda), Serverless Architecture Design, CI/CD Pipeline Setup (with CloudFormation, CodeBuild), NoSQL Optimization (DynamoDB), and REST API Design (API Gateway).. Regarding my professional conduct, I always strived to complete tasks well, comply with regulations, and actively communicate with colleagues to enhance work efficiency.\nTo objectively reflect on my internship process, I would like to self-assess based on the criteria below:\nNo. Criteria Description Excellent (T·ªët) Good (Kh√°) Average (Trung b√¨nh) 1 Knowledge and Technical Skills Understanding of the industry, practical application of knowledge, tool proficiency, quality of work ‚úÖ ‚òê ‚òê 2 Ability to Learn Acquiring new knowledge, fast learning pace ‚úÖ ‚òê ‚òê 3 Proactiveness Self-learning, taking on tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of Responsibility Completing work on time, ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adherence to timing, internal rules, and work processes ‚úÖ ‚òê ‚òê 6 Drive for Improvement Willingness to receive feedback and self-improve ‚úÖ ‚òê ‚òê 7 Communication Presenting ideas, reporting work clearly ‚òê ‚úÖ ‚òê 8 Team Collaboration Working effectively with colleagues, participating in the team ‚úÖ ‚òê ‚òê 9 Professional Conduct Respect for colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-Solving Thinking Identifying issues, proposing solutions, creativity ‚òê ‚úÖ ‚òê 11 Contribution to Project/Organization Work effectiveness, improvement initiatives, team recognition ‚úÖ ‚òê ‚òê 12 Overall Assessment General evaluation of the entire internship process ‚úÖ ‚òê ‚òê Areas for Improvement Improve communication skills to present ideas and report work more clearly and coherently for each team member. Improve the approach to problem-solving thinking. "},{"uri":"https://Kienht7704.github.io/Internship-Report/5-workshop/5.6-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"Clean Up Resources First ‚Äî congratulations on completing this lab! Now let\u0026rsquo;s clean up the resources you created.\nDelete the S3 bucket Disable / Delete the CloudFront distribution After disabling the distribution, AWS will allow you to delete the CloudFront distribution.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Work Environment The work environment is very friendly and open. FCJ team members are always ready to assist when I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, which helps me concentrate better.\n2. Support from Mentor / Team Admin The Mentor provided very detailed guidance, gave clear explanations when I did not understand, and always encouraged me to ask questions. Team admin supported procedures, documentation, and facilitated a smooth working environment for me. The supporting admins were always enthusiastic in reviewing my issues and providing timely solutions.\n3. Alignment between Work and Academic Major The work assigned to me was suitable for the knowledge I acquired at university, while also expanding into new areas that I had not previously encountered. Thanks to this, I both consolidated my foundational knowledge and learned practical skills.\n4. Opportunities for Learning \u0026amp; Skill Development During the internship, I learned many new skills, such as using project management tools, teamwork skills, and professional communication in a corporate environment. The Mentor also shared practical experiences that helped me better define my career direction.\n5. Culture \u0026amp; Team Spirit The company culture is very positive: everyone respects each other, works seriously but remains cheerful. When facing urgent projects, everyone worked together, providing support regardless of position. This made me feel like a part of the collective, even as an intern.\n6. Intern Policies / Benefits The company facilitated flexible working hours when necessary. Furthermore, the opportunity to participate in various events was a major plus.\n"},{"uri":"https://Kienht7704.github.io/Internship-Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://Kienht7704.github.io/Internship-Report/tags/","title":"Tags","tags":[],"description":"","content":""}]